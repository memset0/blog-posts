---
title: æœºå™¨å­¦ä¹ æ¦‚è¿°
slug: /research/note/nndl/introduction
sticker: emoji//1f331
---

## 1. æ•°æ®

æˆ‘ä»¬é€šå¸¸å°†ä¸€ä»½æ ‡è®°å¥½ **ç‰¹å¾(feature)** ä»¥åŠ **æ ‡ç­¾(label)** çš„æ•°æ®çœ‹ä½œä¸€ä¸ª **æ ·æœ¬(sample)**ï¼Œä¸€ç»„æ ·æœ¬ç»„æˆçš„é›†åˆç§°ä¸º **æ•°æ®é›†(data set)**ï¼ä¸€èˆ¬å°†æ•°æ®é›†åˆ†ä¸º **è®­ç»ƒé›†(training set)** å’Œ **æµ‹è¯•é›†(test set)**ï¼å‰è€…ç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼Œåè€…ç”¨æ¥æ£€éªŒæ¨¡å‹å¥½åï¼å¯¹äºä¸€ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬é€šå¸¸ç”¨ä¸€ä¸ª $D$ ç»´çš„ **ç‰¹å¾å‘é‡(feature vector)** æ¥è¡¨ç¤ºå…¶ç‰¹å¾ $\boldsymbol{x}=[x_{1},x_{2},\cdots,x_{D}]^{\top}$ï¼›å¯¹äºå…¶æ ‡ç­¾åˆ™ä¸€èˆ¬ç”¨ä¸€ä¸ªæ ‡é‡ $y$ è¡¨ç¤ºï¼

## 2. æ¨¡å‹

å‡è®¾è®­ç»ƒé›† $\mathcal{D}=\{ (\boldsymbol{x}^{(1)},y^{(1)}),(\boldsymbol{x}^{(2)},y^{(2)}),\cdots, (\boldsymbol{x}^{(N)}, y^{(N)}) \}$ ç”± $N$ ä¸ªæ ·æœ¬ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ **ç‹¬ç«‹åŒåˆ†å¸ƒ(identically and independently distributed, IID)** çš„ï¼Œå³ç‹¬ç«‹åœ°ä»ç›¸åŒçš„æ•°æ®åˆ†å¸ƒä¸­æŠ½å–çš„ï¼æˆ‘ä»¬å¸Œæœ›è®©è®¡ç®—æœºä»ä¸€ä¸ªå‡½æ•°é›†åˆ $\mathcal{F}=\{ f_{1}(\boldsymbol{x}), f_{2}(\boldsymbol{x}), \cdots \}$ ä¸­è‡ªåŠ¨å¯»æ‰¾ä¸€ä¸ªâ€œæœ€ä¼˜â€çš„å‡½æ•° $f^\ast (\boldsymbol{x})$ æ¥è¿‘ä¼¼æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡ $\boldsymbol{x}$ å’Œ $y$ ä¹‹é—´çš„çœŸå®æ˜ å°„å…³ç³»ï¼å¯¹äºä¸€ä¸ªæ ·æœ¬ $\boldsymbol{x}$ï¼Œè®°å½•é€šè¿‡å‡½æ•° $f^{\ast}(\boldsymbol{x})$ é¢„æµ‹çš„å€¼ä¸º $\hat{y}=f^{\ast}(\boldsymbol{x})$ï¼ˆæˆ–è€…æ˜¯æ ‡ç­¾çš„æ¡ä»¶æ¦‚ç‡ $\hat{p}(y\mid \boldsymbol{x})=f^{\ast}_{y}(\boldsymbol{x})$ï¼‰ï¼å¯»æ‰¾è¿™ä¸€å‡½æ•°ä¸€èˆ¬éœ€è¦é€šè¿‡å­¦ä¹ ç®—æ³• $\mathcal{A}$ æ¥å®Œæˆï¼Œè¿™ä¸ªè¿‡ç¨‹å°±ç§°ä¸º **å­¦ä¹ (learning)** æˆ–è€… **è®­ç»ƒ(training)**ï¼

- ä¸è¿‡ï¼Œæœ‰æ—¶å³ä½¿æˆ‘ä»¬æ•°æ®è½»å¾®è¿èƒŒç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾ï¼Œæ¨¡å‹ä»å¯ä»¥è¿è¡Œçš„å¾ˆå¥½ï¼å¦‚åœ¨äººè„¸è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰åº”ç”¨åœºæ™¯å½“ä¸­ï¼

ä½†æ˜¯æˆ‘ä»¬å¹¶ä¸çŸ¥é“è¿™ä¸ªçœŸå®æ˜ å°„å‡½æ•°çš„å…·ä½“å½¢å¼ï¼Œå› è€Œåªèƒ½æ ¹æ®ç»éªŒå‡è®¾ä¸€ä¸ªå‡½æ•°é›†åˆ $\mathcal{F}$ï¼Œè¿™ç§°ä¸º **å‡è®¾ç©ºé—´(hypothesis space)**ï¼Œç„¶åé€šè¿‡è§‚å¯Ÿå…¶åœ¨è®­ç»ƒé›† $\mathcal{D}$ ä¸Šçš„ç‰¹æ€§ï¼Œæ¥é€‰æ‹©ä¸€ä¸ªç†æƒ³çš„ **å‡è®¾(hypothesis)** $f^{\ast}\in \mathcal{F}$ï¼å‡è®¾ç©ºé—´ $\mathcal{F}$ é€šå¸¸æ˜¯ä¸€ä¸ªå‚æ•°åŒ–çš„å‡½æ•°æ— $\mathcal{F}=\{ f(\boldsymbol{x};\theta)\mid \theta \in \mathbb{R} ^{D}\}$ï¼Œå…¶ä¸­ $f(\boldsymbol{x}; \theta)$ æ˜¯å‚æ•°ä¸º $\theta$ çš„å‡½æ•°ï¼Œä¹Ÿç§°ä¸º **æ¨¡å‹(model)**ï¼

- çº¿æ€§æ¨¡å‹çš„å‡è®¾ç©ºé—´ä¸ºä¸€ä¸ªå‚æ•°åŒ–çš„çº¿æ€§å‡½æ•°æ— $f(\boldsymbol{x}; \theta)=\boldsymbol{w}^{\top} \boldsymbol{x}+b$ï¼Œå‚æ•° $\theta$ åŒ…å«æƒé‡å‘é‡ $\boldsymbol{w}$ å’Œåç½® $b$ï¼
- éçº¿æ€§æ¨¡å‹çš„å‡è®¾ç©ºé—´å¯ä»¥å†™ä¸ºå¤šä¸ªéçº¿æ€§ **åŸºå‡½æ•°** $\boldsymbol{\phi}(\boldsymbol{x})$ çš„çº¿æ€§ç»„åˆ $f(\boldsymbol{x}; \theta) = \boldsymbol{w}^{\top} \boldsymbol{\phi}(\boldsymbol{x})+b$ï¼Œå…¶ä¸­ $\boldsymbol{\phi}(\boldsymbol{x})=[\phi_{1}(\boldsymbol{x}),\phi_{2}(\boldsymbol{x}),\cdots,\phi_{K}(\boldsymbol{x})]^{\top}$ ä¸º $K$ ä¸ªéçº¿æ€§åŸºå‡½æ•°æ‰€ç»„æˆçš„å‘é‡ï¼Œå‚æ•° $\theta$ åŒ…å«æƒé‡å‘é‡ $\boldsymbol{w}$ å’Œåç½® $b$ï¼

å¦‚æœéçº¿æ€§æ¨¡å‹çš„åŸºå‡½æ•° $\phi(\boldsymbol{x})$ æœ¬èº«ä¸ºå¯å­¦ä¹ çš„åŸºå‡½æ•°ï¼Œæ¯”å¦‚

$$
\phi_{k}(x) = h(\boldsymbol{w}_{k}^{\top} \phi'(\boldsymbol{x}) +b_{k}),\ \forall 1\le k \le K
$$

å…¶ä¸­ $h(\cdot)$ ä¸ºéçº¿æ€§å‡½æ•°ï¼Œ$\phi'(\boldsymbol{x})$ ä¸ºå¦ä¸€ç»„åŸºå‡½æ•°ï¼Œ$\boldsymbol{w}_{k}$ å’Œ $b_{k}$ ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œåˆ™ $f(\boldsymbol{x}; \theta)$ å°±ç­‰ä»·äºç¥ç»ç½‘ç»œæ¨¡å‹ï¼

## 3. å­¦ä¹ å‡†åˆ™

ä¸Šæ–‡è¯´è¿‡ï¼Œè®­ç»ƒé›†çš„æ ·æœ¬åº”è¯¥æ˜¯ç”±æŸä¸ªå›ºå®šåˆ†å¸ƒ $p_{r}(\boldsymbol{x},y)$ ç‹¬ç«‹åœ°éšæœºäº§ç”Ÿçš„ï¼ä¸€ä¸ªç†æƒ³çš„æ¨¡å‹ $f(\boldsymbol{x}; \theta^{\ast})$ åº”è¯¥åœ¨æ‰€æœ‰çš„ $(\boldsymbol{x},y)$ å–å€¼ä¸Šéƒ½ä¸çœŸå®æ˜ å°„å‡½æ•° $y=g(\boldsymbol{x})$ ä¸€è‡´ï¼Œå³

$$
|f(\boldsymbol{x}; \theta^{\ast}) - y| < \epsilon ,\quad \forall(\boldsymbol{x},y)\in \mathcal{X}\times \mathcal{Y}
$$

æˆ–ä¸çœŸå®æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä¸€è‡´ï¼ˆå…·ä½“çš„å®šä¹‰éœ€è¦ç”¨åˆ° KL æ•£åº¦æˆ–äº¤å‰ç†µï¼›TBDï¼‰ï¼

æ¨¡å‹çš„å¥½åå¯ä»¥é€šè¿‡ **æœŸæœ›é£é™©(expected risk)** $\mathcal{R}(\theta)$ æ¥è¡¡é‡ï¼š

$$
\mathcal{R}(\theta) = \mathbb{E}_{(\boldsymbol{x},y) \sim p_{r} (\boldsymbol{x},y)} [\mathcal{L}(y,f(\boldsymbol{x}; \theta))]
$$

è¿™é‡Œ $\mathcal{L} (y,f(\boldsymbol{x}; \theta))$ ä¸º **æŸå¤±å‡½æ•°(loss function)**ï¼Œç”¨äºé‡åŒ–ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å·®å¼‚ï¼

### 3.1. æŸå¤±å‡½æ•°

ä¸€ä¸ªç†æƒ³çš„æ¨¡å‹åº”è¯¥æœ‰å°½å¯èƒ½å°çš„æœŸæœ›é£é™©ï¼Œä½†ç”±äºä¸çŸ¥é“çœŸå®çš„æ•°æ®åˆ†å¸ƒå’Œæ˜ å°„å‡½æ•°ï¼Œå®é™…ä¸Šæ— æ³•è®¡ç®—æœŸæœ›é£é™© $\mathcal{R}(\theta)$ï¼Œä½†æˆ‘ä»¬å¯ä»¥è®¡ç®—å…¶åœ¨è®­ç»ƒé›† $\mathcal{D}$ ä¸Šçš„ **æœŸæœ›é£é™©(empirical risk)**ï¼Œå³åœ¨è®­ç»ƒé›†ä¸Šçš„å¹³å‡æŸå¤±

$$\mathcal{R}^{\text{emp}}_{D} (\theta) = \dfrac{1}{N} \sum_{n=1}^{N} \mathcal{L}(y^{(n)}, f(\boldsymbol{x}^{(n)}; \theta))$$

æ ¹æ® **ç»éªŒé£é™©æœ€å°åŒ–(empirical risk minimization, ERM)** å‡†åˆ™ï¼æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹å°±æ˜¯è¦æ‰¾åˆ°ä¸€ç»„å‚æ•° $\theta^{\ast}$ ä½¿å¾—ç»éªŒé£é™©æœ€å°ï¼Œå³

$$\theta^{\ast} = \operatorname*{arg min}_{\theta}\  \mathcal{R}_{\mathcal{D}}^{\text{emp}} (\theta)$$

#### 3.1.1. å¹³æ–¹æŸå¤±å‡½æ•°

**å¹³æ–¹æŸå¤±å‡½æ•°(quadratic loss function)** ç»å¸¸ç”¨åœ¨é¢„æµ‹æ ‡ç­¾ $y$ ä¸ºå®æ•°å€¼çš„ä»»åŠ¡ä¸­ï¼š

$$
\mathcal{L}(y,\hat{y}) = \dfrac{1}{2} (y-\hat{y})^{2}
$$

- å¹³æ–¹æŸå¤±å‡½æ•°ä¸€èˆ¬ä¸é€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼

#### 3.1.2. äº¤å‰ç†µæŸå¤±å‡½æ•°

**äº¤å‰ç†µæŸå¤±å‡½æ•°(cross-entropy loss function)** ä¸€èˆ¬ç”¨äºåˆ†ç±»é—®é¢˜ï¼å‡è®¾æ ·æœ¬çš„æ ‡ç­¾ $y\in \{ 1,\cdots,C \}$ ä¸ºç¦»æ•£çš„ç±»åˆ«ï¼Œæ¨¡å‹ $f(\boldsymbol{x}; \theta) \in [0,1]^{C}$ çš„è¾“å‡ºä¸ºç±»åˆ«æ ‡ç­¾çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ $p(y=c\mid \boldsymbol{x}; \theta) = f_{c} (\boldsymbol{x}; \theta)$ï¼è¿™ä¸€è¾“å‡ºåº”æ»¡è¶³æ¦‚ç‡åˆ†å¸ƒçš„æ€§è´¨ï¼Œå³ $f_{c} (\boldsymbol{x}; \theta) \geq 0,\space \forall 1\le c \le C$ ä¸” $\sum_{c=1}^{C} f_c (\boldsymbol{x}; \theta) = 1$ï¼

æ ‡ç­¾çš„çœŸå®åˆ†å¸ƒ $\boldsymbol{y}$ å’Œæ¨¡å‹é¢„æµ‹åˆ†å¸ƒ $\hat{\boldsymbol{y}}=f(\boldsymbol{x}; \theta)$ ä¹‹é—´çš„äº¤å‰ç†µä¸º

$$
\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}}) = -\boldsymbol{y}^{\top} \log \hat{\boldsymbol{y}} = - \sum_{c=1}^{C} y_{c} \log \hat{y}_{c}
$$

#### 3.1.3. Hinge æŸå¤±å‡½æ•°

å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œå‡è®¾ $y\in\{ -1,+1 \}$ï¼Œ$f(\boldsymbol{x}; \theta)\in \mathbb{R}$ï¼Œå®šä¹‰ **Hinge æŸå¤±å‡½æ•°(Hinge loss function)** ä¸º

$$
\mathcal{L}(y, \hat{y}) = \max(0,1-y f(\boldsymbol{x}; \theta))
$$

### 3.2. æ­£åˆ™åŒ–

é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬ä¼šåŒ…å«ä¸€å®šçš„å™ªå£°æ•°æ®ï¼Œä¸èƒ½å¾ˆå¥½çš„ååº”åŸå§‹æ•°æ®çš„çœŸå®æƒ…å†µï¼Œè¿™å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ï¼ä¸€èˆ¬åœ¨ç»éªŒé£é™©æœ€å°åŒ–çš„åŸºç¡€ä¸Šå†å¼•å…¥å‚æ•°çš„ **æ­£åˆ™åŒ–(regularization)** æ¥é™åˆ¶æ¨¡å‹èƒ½åŠ›ï¼Œä½¿å…¶ä¸è¦è¿‡åº¦åœ°æœ€å°åŒ–ç»éªŒé£é™©ï¼è¿™ä¸€å‡†åˆ™ç§°ä¸º **ç»“æœé£é™©æœ€å°åŒ–(structure risk minimization risk)** å‡†åˆ™ï¼

$$
\theta^{\ast} = \operatorname*{arg min}_{\theta}\ \mathcal{R}^{\text{struct}}_{\mathcal{D}} (\theta) = \operatorname*{arg min}_{\theta}\ \mathcal{R}^{\text{emp}}_{\mathcal{D}} + \dfrac{1}{2} \lambda ||\theta||^{2}
$$

è¿™é‡Œ $||\theta||$ æ˜¯ä½¿ç”¨ $L_{2}$ èŒƒæ•°çš„æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºå‡å°‘å‚æ•°ç©ºé—´ï¼Œ$\lambda$ å¯ä»¥æ§åˆ¶æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸‹é¢æåˆ°çš„å…¶ä»–æ­£åˆ™åŒ–é¡¹ï¼

#### 3.2.1. L1 æ­£åˆ™åŒ–

> [!summary] å®šä¹‰ï¼š**èŒƒæ•°(norm)**
>
> çº¿æ€§ä»£æ•°ä¸­çš„èŒƒæ•°æ˜¯æŒ‡å°†å‘é‡æ˜ å°„åˆ°æ ‡é‡çš„å‡½æ•° $f$ï¼Œå¯¹äºä»»æ„å‘é‡ $\boldsymbol{x}$ï¼ŒèŒƒæ•° $f(x)$ æ»¡è¶³ä¸€ä¸‹æ€§è´¨ï¼š
>
> -   $f(\alpha \boldsymbol{x}) = |\alpha|f(\boldsymbol{x})$ï¼›
> -   ä¸‰è§’ä¸ç­‰å¼ï¼š$f(\boldsymbol{x}+\boldsymbol{y})\le f(\boldsymbol{x})+f(\boldsymbol{y})$ï¼›
> -   éè´Ÿæ€§ï¼š$f(\boldsymbol{x})\ge 0$ï¼
>
> $L_p$ èŒƒæ•°è¢«å®šä¹‰ä¸ºï¼š
>
> $$
> ||\boldsymbol{x}||_{p} = \left(  \sum_{i=1}^n |x_{i}|^p \right)^{1/p}
> $$

$L_{1}$ æ­£åˆ™åŒ–å³ä½¿ç”¨ $L_{1}$ èŒƒæ•°çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå³åœ¨ç›®æ ‡å‡½æ•°ä¸­æ·»åŠ  $L_{1}$ æ­£åˆ™é¡¹ï¼š

$$
||\boldsymbol{x}||_{1} = \sum_{i=1}^{d} |x_{i}|
$$

- ä½¿ç”¨ $L_1$ æ­£åˆ™åŒ–çš„çº¿æ€§æ¨¡å‹è¢«ç§°ä¸º **Lasso å›å½’(lasso regression)**ï¼
- åº”ç”¨ $L_1$ èŒƒæ•°è¿›è¡Œæƒ©ç½šä¼šå¯¼è‡´æ¨¡å‹å°†æƒé‡é›†ä¸­åœ¨ä¸€å°éƒ¨åˆ†ç‰¹å¾ä¸Šï¼Œè€Œå°†å…¶ä»–æƒé‡æ¸…é™¤ä¸ºé›¶ï¼è¿™ç§°ä¸º **ç‰¹å¾é€‰æ‹©(feature selection)**ï¼

#### 3.2.2. L2 æ­£åˆ™åŒ–

$L_{2}$ æ­£åˆ™åŒ–å³ä½¿ç”¨ $L_2$ èŒƒæ•°çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå³åœ¨ç›®æ ‡å‡½æ•°ä¸­æ·»åŠ  $L_{2}$ æ­£åˆ™é¡¹ï¼š

$$
||\boldsymbol{x}||_{2} = \sqrt{\sum_{i=1}^{d} x_{i}^{2}}
$$

- ä½¿ç”¨ $L_{2}$ æ­£åˆ™åŒ–çš„çº¿æ€§æ¨¡å‹è¢«ç§°ä¸º **å²­å›å½’(ridge regression)**ï¼
- $L_{2}$ èŒƒæ•°ä¼šå¯¹æƒé‡å‘é‡çš„å¤§åˆ†é‡æ–½åŠ å·¨å¤§çš„æƒ©ç½šï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„å­¦ä¹ ç®—æ³•åå‘äºåœ¨å¤§é‡ç‰¹å¾ä¸Šå‡åŒ€åˆ†å¸ƒæƒé‡çš„æ¨¡å‹ï¼Œä»è€Œä½¿å¾—å®ƒä»¬å¯¹å•ä¸ªå˜é‡ä¸­çš„è§‚æµ‹è¯¯å·®æ›´ä¸ºç¨³å®šï¼

## 4. ä¼˜åŒ–ç®—æ³•

### 4.1. éšæœºæ¢¯åº¦ä¸‹é™

**æ¢¯åº¦ä¸‹é™(gradient descent)** æ³•æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€ç®€å•ã€æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼å®ƒé€šè¿‡ä¸æ–­åœ°åœ¨æŸå¤±å‡½æ•°é€’å‡çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°æ¥é™ä½è¯¯å·®ï¼ä¸€ä¸ªæœ€ç®€å•çš„æƒ³æ³•æ˜¯æ²¿ç€æ¢¯åº¦å‡å°çš„æ–¹å‘æ›´æ–°æˆ‘ä»¬çš„å‚æ•°ï¼Œé€šè¿‡ä¸æ–­è¿­ä»£çš„æ–¹å¼è¾¾åˆ°ä¸€ä¸ªæœ€å°å€¼ç‚¹ï¼

ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬ä¼šåœ¨æ¯æ¬¡è¿­ä»£çš„æ—¶å€™éšæœºæŠ½å–ä¸€å°æ‰¹æ ·æœ¬ $\mathcal B$ï¼Œè¿™ç§æ–¹æ³•ç§°ä¸º **å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™(minibatch stochastic gradient descent)** æˆ–è€… **éšæœºæ¢¯åº¦ä¸‹é™(stochastic gradient descent, SGD)**ï¼

$$
(\boldsymbol{w}', b') \leftarrow (\boldsymbol{w},b) - \dfrac{\eta}{|\mathcal B|} \sum_{i\in \mathcal B} {\partial \ l^{(i) } (\boldsymbol{w},b) \over \partial  (\boldsymbol{w},b)}
$$

è¿™é‡Œ $|\mathcal B|$ è¡¨ç¤º **æ‰¹é‡å¤§å°(batch size)**ï¼Œå³æ¯ä¸ªå°æ‰¹é‡æ•°æ®çš„å¤§å°ï¼›$\eta$ è¡¨ç¤º **å­¦ä¹ ç‡(learning rate)**ï¼è¿™ç§ä¸€èˆ¬æ¥è¯´æ˜¯é€šè¿‡é¢„å…ˆåˆ¶å®šï¼Œè¿™ç§å¯ä»¥è°ƒæ•´ä½†ä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°çš„å‚æ•°ç§°ä¸º **è¶…å‚æ•°(hyperparameter)**ï¼ è¶…å‚æ•°é€šå¸¸æ˜¯æˆ‘ä»¬æ ¹æ®è®­ç»ƒè¿­ä»£ç»“æœæ¥è°ƒæ•´çš„ï¼Œè€Œè®­ç»ƒè¿­ä»£ç»“æœæ˜¯åœ¨ç‹¬ç«‹çš„ **éªŒè¯æ•°æ®é›†(validation dataset)** ä¸Šè¯„ä¼°å¾—åˆ°çš„ï¼

- æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬å¾—åˆ°çš„å‚æ•°èƒ½å¤Ÿåœ¨ä»æœªè§è¿‡çš„æ•°æ®ä¸Šå®ç°è¾ƒä½çš„æŸå¤±ï¼Œè¿™ä¸€æŒ‘æˆ˜è¢«ç§°ä¸º **æ³›åŒ–(generalization)**ï¼
- ä¸ºäº†å‡å°‘è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œå¯ä»¥é‡‡ç”¨ä¸€ç§ç§°ä¸º **æå‰åœæ­¢(early stop)** çš„ç­–ç•¥ï¼šæ¯æ¬¡è¿­ä»£åæµ‹è¯•å¾—åˆ°çš„æ¨¡å‹ $f(\boldsymbol{x}; \theta)$ åœ¨æµ‹è¯•é›†ä¸Šçš„é”™è¯¯ç‡ï¼å¦‚æœé”™è¯¯ç‡ä¸å†ä¸‹é™ï¼Œå°±åœæ­¢è¿­ä»£ï¼

## 5. ç»å…¸å›å½’é—®é¢˜

### 5.1. çº¿æ€§å›å½’

åœ¨çº¿æ€§å›å½’é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å‡è®¾è¾“å‡º $y$ æ˜¯è¾“å…¥ç‰¹å¾ $\boldsymbol{x}\in\mathbb{R}^{d}$ çš„ä¸€ä¸ªä»¿å°„å˜æ¢ï¼æˆ‘ä»¬å¾€å¾€ä¼šç»™å‡º $n$ å¯¹æ ·æœ¬ $(\boldsymbol{x}_{i},y_{i})$ï¼Œå¯ä»¥ç”¨ $\boldsymbol{X} \in \mathbb{R}^{n\times d}$ å’Œ $\boldsymbol{y}\in\mathbb{R}^{n}$ æ ·æœ¬é›†åˆï¼Œå¹¶å¯»æ‰¾æƒé‡ $\boldsymbol{w}$ å’Œåç½® $b$ï¼Œä½¿å¾—é¢„æµ‹ç»“æœ $\hat{y}=w_{1} x_{1} + \dots + w_{d} x_{d} + b = \boldsymbol{w}^{\text{T}}\boldsymbol{x} + b$ å°½å¯èƒ½åœ°æ¥è¿‘çœŸå®å€¼ï¼

![qwEZq3QH.png|263](https://img.memset0.cn/2024/08/17/qwEZq3QH.png)

ä¸ºäº†è¡¡é‡é¢„æµ‹ç»“æœå’ŒçœŸå®å€¼çš„æ¥è¿‘ç¨‹åº¦ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥å¹³æ–¹è¯¯å·®çš„æ¦‚å¿µï¼ˆè¿™é‡Œå¼•å…¥çš„ $\frac 12$ æ˜¯ä¸ºäº†æ–¹ä¾¿å¾—åˆ°æ›´ç®€æ´çš„å¯¼æ•°å½¢å¼ï¼‰ï¼š

$$
l^{(i)} (\boldsymbol{w},b) = \dfrac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right) ^{2}
$$

å¦å¤–ï¼Œåœ¨çœŸå®æ•°æ®é›†ä¸­ï¼Œæ— è®ºæˆ‘ä»¬ç”¨ä»€ä¹ˆæ‰‹æ®µæ¥è§‚æµ‹ç‰¹å¾ $\boldsymbol{X}$ å’Œæ ‡ç­¾ $\boldsymbol{y}$ï¼Œéƒ½éš¾å…ä¼šäº§ç”Ÿå™ªå£°é¡¹ï¼æˆ‘ä»¬éœ€è¦å¼•å…¥ä¸€ä¸ªå™ªå£°é¡¹æ¥è€ƒè™‘å½±å“ï¼è¿™é‡Œï¼Œæˆ‘ä»¬å…ˆå‡è®¾å™ªå£°é¡¹æ˜¯æœä»**é«˜æ–¯åˆ†å¸ƒ(Gaussian distribution)**ï¼ˆä¹Ÿç§°ä¸º **æ­£æ€åˆ†å¸ƒ(normal distribution)**ï¼‰çš„ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š

$$
p(x)  =\dfrac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\dfrac{1}{2\sigma^2} (x-\mu)^2 \right) ,\quad\text{where } x \sim N(\mu,\sigma^{2})
$$

å¯ä»¥è¯æ˜ï¼šåœ¨é«˜æ–¯å™ªå£°çš„å‡è®¾ä¸‹ï¼Œæœ€å°åŒ–å‡æ–¹è¯¯å·®ç­‰ä»·äºå¯¹çº¿æ€§æ¨¡å‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨ä¸Šæ–‡é€‰æ‹©äº†å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼

> [!quote]- è¯æ˜ï¼šæœ€å°åŒ–å‡æ–¹è¯¯å·®ç­‰ä»·äºå¯¹çº¿æ€§æ¨¡å‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡
>
> æˆ‘ä»¬å‡è®¾è§‚æµ‹ä¸­åŒ…å«å™ªå£°ï¼Œå…¶ä¸­å™ªå£°æœä»æ­£æ€åˆ†å¸ƒï¼å™ªå£°æ­£æ€åˆ†å¸ƒå¦‚ä¸‹å¼ï¼š
>
> $$
> y = \boldsymbol{w}^\text T \boldsymbol{x} +b +\epsilon
> $$
>
> å…¶ä¸­ï¼Œ$\epsilon \sim \mathcal N(0, \sigma^2)$ï¼å› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»™å®šçš„ $\boldsymbol{x}$ è§‚æµ‹åˆ°ç‰¹å®š $y$ çš„ **ä¼¼ç„¶(likelihood)**ï¼š
>
> $$
> P(y\mid\boldsymbol x) = \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\dfrac{1}{2\sigma^2} (y-\boldsymbol w^\text T \boldsymbol x - b)^2 \right)
> $$
>
> ç°åœ¨ï¼Œæ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æ³•ï¼Œå‚æ•° $\boldsymbol w$ å’Œ $b$ çš„æœ€ä¼˜å€¼æ˜¯ä½¿æ•´ä¸ªæ•°æ®é›†çš„ä¼¼ç„¶æœ€å¤§çš„å€¼ï¼š
>
> $$
> P(y\mid\boldsymbol X) = \prod_{i=1}^n p(y^{(i)} \mid \boldsymbol x^{(i)})
> $$
>
> æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æ³•é€‰æ‹©çš„ä¼°è®¡é‡ç§°ä¸º **æå¤§ä¼¼ç„¶ä¼°è®¡é‡**ï¼è¿™é‡Œå¤„ç†è¿ä¹˜æ¯”è¾ƒå›°éš¾ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å–å¯¹æ•°æ¥ç®€åŒ–ï¼ç”±äºå†å²åŸå› ï¼Œä¼˜åŒ–é€šå¸¸è¯´çš„æ˜¯æœ€å°åŒ–è€Œä¸æ˜¯æœ€å¤§åŒ–ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å‘¢å¯ä»¥æ”¹ä¸ºæ±‚æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ $-\log P(y\mid \boldsymbol X)$ï¼Œå› æ­¤å¯ä»¥å¾—åˆ°ï¼š
>
> $$
> -\log P(y\mid \boldsymbol X) = \sum_{i=1}^n \dfrac{1}{2} \log(2\pi\sigma^2) + \dfrac{1}{2\sigma^2} \left(y^{(i)} - \boldsymbol w^\text T \boldsymbol x^{(i)} - b\right)^2
> $$
>
> æˆ‘ä»¬åªéœ€è¦å‡è®¾ $\sigma$ æ˜¯å›ºå®šå¸¸æ•°å°±å¯ä»¥å¿½ç•¥è¿™æ ·ç¬¬ä¸€é¡¹ï¼Œè¿™æ ·ååŠéƒ¨åˆ†å°±æ˜¯å‡æ–¹è¯¯å·®äº†ï¼è¿™å°±è¯´æ˜äº†åœ¨é«˜æ–¯å™ªå£°çš„å‡è®¾ä¸‹ï¼Œæœ€å°åŒ–å‡æ–¹è¯¯å·®ç­‰ä»·äºå¯¹çº¿æ€§æ¨¡å‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œçº¿æ€§å›å½’é—®é¢˜æ˜¯å­˜åœ¨ **è§£æè§£(analytical solution)** çš„ï¼š$\boldsymbol{w}^{\ast} = (\boldsymbol{X}^{\text{T}} \boldsymbol{X})^{-1} \boldsymbol{X}^{\text{T}} \boldsymbol{y}$ï¼Œä¸è¿‡è¿™å¯¹æˆ‘ä»¬ç ”ç©¶æœºå™¨å­¦ä¹ é—®é¢˜æ²¡æœ‰å¸®åŠ©â€”â€”åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å„ç§æ•°å€¼æ–¹æ³•æ¥è§£å†³å›å½’é—®é¢˜ï¼

### 5.2. softmax å›å½’

ä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹çš„è¾“å‡º $\hat{y}_j$ å¯ä»¥è§†ä¸ºå±äºç±» $j$ çš„æ¦‚ç‡ï¼Œç„¶åé€‰æ‹©å…·æœ‰æœ€å¤§è¾“å‡ºå€¼çš„ç±»åˆ« $\operatorname*{argmax}\limits_j y_j$ ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹ï¼

ä½†æ˜¯æˆ‘ä»¬å¹¶ä¸èƒ½ç›´æ¥å°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹ $o$ ç›´æ¥ä½œä¸ºè¾“å‡ºï¼å› ä¸ºæˆ‘ä»¬å¹¶æ²¡æœ‰å¯¹çº¿æ€§å±‚çš„è¾“å‡ºè¿›è¡Œä¸€äº›é™åˆ¶ï¼šæ²¡æœ‰é™åˆ¶æ‰€æœ‰æ¦‚ç‡çš„æ€»å’Œä¸º $1$ï¼Œæ²¡æœ‰é™åˆ¶æ¦‚ç‡éƒ½æ˜¯éè´Ÿçš„ï¼

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè®­ç»ƒçš„ç›®æ ‡å‡½æ•°ï¼Œæ¥æ¿€åŠ±æ¨¡å‹ç²¾å‡†åœ°ä¼°è®¡æ¦‚ç‡ï¼ä¾‹å¦‚åœ¨åˆ†ç±»å™¨è¾“å‡º $0.5$ çš„æ‰€æœ‰æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™äº›æ ·æœ¬å®¤åˆšå¥½æœ‰ä¸€åŠå®é™…ä¸Šå±äºé¢„æµ‹çš„ç±»åˆ«ï¼Œè¿™ä¸ªå±æ€§è¢«ç§°ä¸º **æ ¡å‡†(calibration)**ï¼

softmax å‡½æ•°å°±è¢«è®¾è®¡ç”¨äºå®ç°è¿™ä¸€åŠŸèƒ½ï¼šå®ƒèƒ½å¤Ÿå°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹è½¬åŒ–ä¸ºéè´Ÿæ•°å¹¶ä¸”æ€»å’Œä¸º $1$ï¼ŒåŒæ—¶ä½¿æ¨¡å‹ä¿æŒå¯å¯¼çš„æ€§è´¨ï¼

$$
\hat{\boldsymbol{y}} = \operatorname{softmax}(\boldsymbol{o}) ,\quad\text{where } \hat{y}_j = \frac{\exp (o_j)}{\sum_k \exp (o_k)}.
$$

æ˜¾ç„¶ï¼Œè¿™é‡Œå¯¹äºæ‰€æœ‰çš„ $j$ éƒ½æœ‰ $\hat{y}_j\in [0,1]$ ä¸” $\sum_j \hat{y}_j=1$ï¼Œæ•… $\hat{\boldsymbol{y}}$ å¯ä»¥è§†ä¸ºä¸€ä¸ªæ­£ç¡®çš„æ¦‚ç‡åˆ†å¸ƒï¼ä¸” softmax å‡½æ•°ä¸æ”¹å˜å¯¹åº”ä½ç½®å…ƒç´ çš„å¤§å°å…³ç³»ï¼Œæ•… $\displaystyle{\operatorname*{argmax}_j \hat{y}_j = \operatorname*{argmax}_j o_j}$ï¼

## 6. å°ç»“

> [!quote] [P50](zotero://open-pdf/library/items/KZDJW43E?page=50&annotation=7HGPHUN6) | é‚±é”¡é¹ï¼Œç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ï¼Œæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œhttps://nndl.github.io/, 2020.
>
> æœ¬ç« ç®€å•åœ°ä»‹ç»äº†æœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶ä¸ºåé¢ä»‹ç»çš„ç¥ç»ç½‘ç»œè¿›è¡Œä¸€äº›ç®€å•çš„é“ºå«æœºå™¨å­¦ä¹ ç®—æ³•è™½ç„¶ç§ç±»ç¹å¤šï¼Œä½†å…¶ä¸­ä¸‰ä¸ªåŸºæœ¬çš„è¦ç´ ä¸ºï¼šæ¨¡å‹ã€å­¦ä¹ å‡†åˆ™ã€ä¼˜åŒ–ç®—æ³•å¤§éƒ¨åˆ†çš„æœºå™¨å­¦ä¹ ç®—æ³•éƒ½å¯ä»¥çœ‹ä½œè¿™ä¸‰ä¸ªåŸºæœ¬è¦ç´ çš„ä¸åŒç»„åˆï¼ç›¸åŒçš„æ¨¡å‹ä¹Ÿå¯ä»¥æœ‰ä¸åŒçš„å­¦ä¹ ç®—æ³•ï¼æ¯”å¦‚çº¿æ€§åˆ†ç±»æ¨¡å‹æœ‰æ„ŸçŸ¥å™¨ã€Logistic å›å½’å’Œæ”¯æŒå‘é‡æœºï¼Œå®ƒä»¬ä¹‹é—´çš„å·®å¼‚åœ¨äºä½¿ç”¨äº†ä¸åŒçš„å­¦ä¹ å‡†åˆ™å’Œä¼˜åŒ–ç®—æ³•ï¼
>
> ç›®å‰æœºå™¨å­¦ä¹ ä¸­æœ€ä¸»æµçš„ä¸€ç±»æ–¹æ³•æ˜¯ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼Œå°†æœºå™¨å­¦ä¹ é—®é¢˜çœ‹ä½œç»Ÿè®¡æ¨æ–­é—®é¢˜ï¼Œå¹¶ä¸”åˆå¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºé¢‘ç‡å­¦æ´¾å’Œè´å¶æ–¯å­¦æ´¾é¢‘ç‡å­¦æ´¾å°†æ¨¡å‹å‚æ•° ğœƒ çœ‹ä½œå›ºå®šå¸¸æ•°ï¼›è€Œè´å¶æ–¯å­¦æ´¾å°†å‚æ•° ğœƒ çœ‹ä½œéšæœºå˜é‡ï¼Œå¹¶ä¸”å­˜åœ¨æŸç§å…ˆéªŒåˆ†å¸ƒï¼æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªé‡è¦å†…å®¹æ˜¯è¡¨ç¤ºå­¦ä¹ ï¼

## 7. å‚è€ƒèµ„æ–™

- [ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹  (nndl.github.io)](https://nndl.github.io/)
- [3.1. çº¿æ€§å›å½’ â€” åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  2.0.0 documentation (d2l.ai)](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)
- [3.4. softmax å›å½’ â€” åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  2.0.0 documentation (d2l.ai)](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)
- [11.1. ä¼˜åŒ–å’Œæ·±åº¦å­¦ä¹  â€” åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  2.0.0 documentation (d2l.ai)](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)
