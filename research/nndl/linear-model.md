---
title: 线性模型
sync: /research/nndl/linear-model.md
sticker: emoji//1f684
---

## 2. 优化问题

### 2.1. 目标函数

为了度量模型在整个数据集上的质量，我们可以将损失函数设定为模型在 $n$ 个样本上的损失均值（也相当于是求和）：

$$
\mathcal L(\boldsymbol{w}, b) = \dfrac{1}{n} \sum_{i=1}^n l^{(i)} (\boldsymbol{w},b)
$$

训练模型的过程就是我们想要确定一组参数 $(\boldsymbol{w}^\ast,b^\ast)$，从而最小化在所有样本上的损失均值，也就是：

$$
(\boldsymbol{w}^\ast, b^\ast) = \operatorname*{argmin}_{\boldsymbol{w},b} \mathcal L(\boldsymbol{w}, b)
$$

### 2.2. 全连接层

当我们的模型需要支持多个输出时，我们一般会选择将每个输入都和每一个输出建立联系，这样的单层神经网络称为 **全连接层**．我们以一个 $4$ 输入 $3$ 输出的基于线性模型的单层神经网络为例：

![MaAKC9P3.png|393](https://static.memset0.cn/img/v6/2024/08/15/MaAKC9P3.png)

其对应的数学表达即：

$$
\begin{cases}
o_{1} = x_{1} w_{11} + x_{2} w_{12} + x_{3} w_{13} + x_{4} w_{14} + b_{1},  \\
o_{2} = x_{1} w_{21} + x_{2} w_{22} + x_{3} w_{23} + x_{4} w_{24} + b_{1},  \\
o_{3} = x_{1} w_{31} + x_{2} w_{32} + x_{3} w_{33} + x_{4} w_{34} + b_{1}.  \\
\end{cases} \quad\Longleftrightarrow\quad
\boldsymbol{o} =\boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}
$$

对于一个具有 $d$ 个输入和 $q$ 个输出的全连接层，参数开销为 $\mathcal O(dq)$．不过幸运的是，我们往往可以指定一个超参数 $n$，使得将 $d$ 个输入转换为 $q$ 个输出的成本降低到 $\mathcal O(\frac{dq}{n})$，这有助于在实际应用中平衡参数节约和模型有效性．

## 3. 数值优化的挑战

### 3.1. 局部最小值

对于一些目标函数，其可能存在局部最小值．在局部最小值处或附近，目标函数解的梯度接近或变为零，这将导致我们的算法最终陷在局部最小值中，而不能找到全局最小值．

![Fhi8spBj.png|297](https://static.memset0.cn/img/v6/2024/08/17/Fhi8spBj.png)

事实上，这也是后文提到的 [[2. 线性模型#5.1. 随机梯度下降|随机梯度下降]] 算法的优势之一，因为我们每次都选择一小批量数据，我们可以自然地从局部最小值中跳出．

### 3.2. 鞍点

**鞍点(saddle point)** 指的是函数的所有梯度都消失但既不是局部最小值更不是全局最小值的位置：

| $f(x)=x^{3}$                                                                                                                                                 | $f(x,y)=x^{2}-y^{2}$                                                                                                                                                                                                           |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ![](https://static.memset0.cn/img/v6/2024/08/17/1W9YR2hi.png)<br>注意到在 $x=0$ 处，函数的一阶导数和二阶导数均为 $0$，这时优化可能停止，即使其并不是最小值． | ![](https://static.memset0.cn/img/v6/2024/08/17/D2wSw46O.png)<br>这一函数的鞍点更加隐蔽，注意到在 $(0,0)$ 的位置，所有方向的导数都消失，但这是关于 $y$ 的最小值，也是关于 $x$ 的最大值，且显然不是我们想要寻找的全局最小值点． |

### 3.3. 梯度消失

设我们使用 $f(x)=\tanh(x)$ 作为激活函数时，其导数为 $f'(x)= 1-\tanh^{2}(x)$．在距离 $x=0$ 较远的位置，函数的梯度会变得很小甚至消失（如 $f'(4)=0.0013$）．这也是引入 ReLU 激活函数之前训练深度学习模型相当棘手的原因之一．

![FKXyGj2O.png|313](https://static.memset0.cn/img/v6/2024/08/17/FKXyGj2O.png)

### 3.4. 小结

此外，在多层感知机中，因为多层神经网络的结构，我们也会遇到 [[3. 前馈神经网络#7.1. 梯度消失|梯度消失]] 和 [[3. 前馈神经网络#梯度爆炸|梯度爆炸]] 的问题．
