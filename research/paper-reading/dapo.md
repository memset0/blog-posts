---
# 
title: "ã€Œè®ºæ–‡ç²¾è¯» #21ã€DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
create-date: 2025-03-21 11:34:22
update-date: 2025-03-21 11:34:22
link-chat: https://chat.memset0.cn/chat?session=ssn_uaicSGbAMyYz&topic=tpc_UKZ97tf6nrWq
slug: /research/paper-reading/dapo
# indexed: true
tags:
  - Reinforcement-Learning
  - PPO
  - GRPO
  - DAPO
  - topic/web-agent
# 
citekey: yuDAPOOpenSourceLLM2025
doi: "10.48550/arXiv.2503.14476" 
export-date: 2025-03-21 19:50:36
---



## 1. Background

### 1.1. Proximal Policy Optimization (PPO)

- [ã€Œè®ºæ–‡ç²¾è¯» #24ã€Proximal Policy Optimization Algorithms](/research/paper-reading/ppo/)

**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization, PPO)** æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºè®­ç»ƒæ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

PPO çš„ç›®æ ‡å‡½æ•°æ˜¯æœ€å¤§åŒ–ï¼š

$$
{\mathcal{J}}_{\mathrm{{PPO}}}\left( \theta \right) = {\mathbb{E}}_{\left( {q,a}\right) \sim \mathcal{D},{o}_{ \leq t} \sim {\pi }_{{\theta }_{\text{old }}}\left( {\cdot \mid q}\right) }\left\lbrack {\min \left( {\frac{{\pi }_{\theta }\left( {{o}_{t} \mid q,{o}_{ < t}}\right) }{{\pi }_{{\theta }_{\text{old }}}\left( {{o}_{t} \mid q,{o}_{ < t}}\right) }{\widehat{A}}_{t},\operatorname{clip}\left( {\frac{{\pi }_{\theta }\left( {{o}_{t} \mid q,{o}_{ < t}}\right) }{{\pi }_{{\theta }_{\text{old }}}\left( {{o}_{t} \mid q,{o}_{ < t}}\right) },1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{t}}\right) }\right\rbrack
$$

> -   $(q,a)\sim \mathcal{D}$ï¼šä» data åˆ†å¸ƒä¸­éšæœºé‡‡æ ·çš„ question-answer å¯¹ã€‚
> -   $o_{\leq t} \sim \pi_{\theta_{\text{old}}} (\cdot|q)$ï¼šä½¿ç”¨æ—§ç­–ç•¥ï¼Œæ ¹æ®é—®é¢˜ $q$ ç”Ÿæˆå‰ $t$ æ­¥çš„ tokensã€‚
> -   $\pi_{\theta}(o_{t}|q,o_{<t})$ï¼šåœ¨ç»™å®š $q$ å’Œå‰ $t-1$ æ­¥æ“ä½œçš„ tokens $o_{<t}$ çš„æƒ…å†µä¸‹ï¼Œå½“å‰ç­–ç•¥ $\pi_{\theta}$ è‰²èŠ±å§‘å¨˜æˆ token $o_{t}$ çš„æ¦‚ç‡ã€‚
> -   $\displaystyle{\dfrac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{\text{old}}}(o_{t}|q,o_{<t})}}$ï¼š**é‡è¦æ€§é‡‡æ ·æ¯”ç‡(importance sampling ratio)**ï¼Œæ–°æ—§ç­–ç•¥ç”Ÿæˆ token $o_{t}$ çš„æ¦‚ç‡æ¯”ä¾‹ï¼Œè¡¡é‡äº†æ–°æ—§ç­–ç•¥çš„å·®å¼‚ã€‚
> -   $\hat{A}_{t}$ï¼š**ä¼˜åŠ¿å‡½æ•°(advantage function)** çš„ä¼°è®¡å€¼ï¼Œè¿™é‡Œä½¿ç”¨ **å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(Generalized Advantage Estimation, GAE)** çš„æ–¹æ³•è®¡ç®—ã€‚
> -   $\text{clip}\left(\dfrac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}, 1 - \varepsilon, 1 + \varepsilon\right)$ï¼š**è£å‰ª(clip)** æ˜¯ PPO çš„æ ¸å¿ƒæ€æƒ³ï¼Œå½“é‡è¦é‡‡æ ·æ€§æ¯”ç‡è¶…è¿‡ $[1-\epsilon,1+\epsilon]$ çš„èŒƒå›´å†…æ—¶ï¼Œå°†å…¶è£å‰ªåˆ°è¿™ä¸ªèŒƒå›´å†…ã€‚

### 1.2. Group Relative Policy Optimizati(GRPO)

- [ã€Œè®ºæ–‡ç²¾è¯» #22ã€DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](/research/paper-reading/deepseek-math/)

> -   ğŸ‘ åˆ©ç”¨åŒä¸€é—®é¢˜ä¸‹ä¸åŒå›ç­”çš„ç›¸å¯¹å¥½åç¨‹åº¦æ¥æŒ‡å¯¼ç­–ç•¥çš„æ›´æ–°ï¼Œæ›´é€‚ç”¨äºå¥–åŠ±ä¿¡å·ç¨€ç–æˆ–éš¾ä»¥è®¾è®¡çš„åœºæ™¯ã€‚

**ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)** çš„æ ¸å¿ƒæ€æƒ³åœ¨äºæ‘’å¼ƒäº† PPO çš„ä»·å€¼å‡½æ•°è®¾è®¡ï¼Œè½¬è€Œ==åˆ©ç”¨åŒä¸€é—®é¢˜ä¸‹ä¸åŒå›ç­”ä¹‹é—´çš„ç›¸å¯¹å¥½åç¨‹åº¦==æ¥ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ã€‚

å¯¹äºç»™å®šçš„ question-answer å¯¹ $(q,a)$ï¼Œæ—§è¡Œä¸ºç­–ç•¥ $\pi_{\theta_{\text{old}}}$ ä¼šå¯¹è¿™ä¸€é—®é¢˜é‡‡æ ·ä¸€ **ç»„(group)** $G$ ä¸ªä¸åŒçš„å›ç­” $\{ o_{i} \}_{i=1}^{G}$ã€‚æ¥ç€ï¼ŒGRPO ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—ç¬¬ $i$ ä¸ªå›ç­”çš„ä¼˜åŠ¿å‡½æ•°ï¼š

$$
{\widehat{A}}_{i,t} = \frac{{R}_{i} - \operatorname{mean}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }{\operatorname{std}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }
$$

> -   $\text{mean}(\cdot)$ è¡¨ç¤ºå¹³å‡å€¼ï¼Œ$\text{std}(\cdot)$ è¡¨ç¤ºæ ‡å‡†å·®ã€‚
> -   ä¼˜åŠ¿å‡½æ•° $\widehat{A}_{i,t}$ å¯ä»¥çœ‹åšå¯¹ç›´æ¥å¥–åŠ±çš„ **æ ‡å‡†åŒ–(standardization)**ï¼Œé€šè¿‡å°†å‡å€¼å¹³ç§»åˆ° $0$ï¼Œæ ‡å‡†å·®ç¼©æ”¾åˆ° $1$ã€‚
> -   ç‰¹åˆ«åœ°ï¼Œæœ¬ç®—æ³•ï¼ˆDAPOï¼‰ä¼šç›´æ¥è¿‡æ»¤æ‰æ‰€æœ‰å›ç­”å¥–åŠ±ç›¸åŒçš„æƒ…å†µï¼Œä»¥é¿å…é™¤ä»¥ $0$ çš„é—®é¢˜ã€‚

$$
\begin{gathered}
{\mathcal{J}}_{\text{GRPO }}\left( \theta \right) = {\mathbb{E}}_{\left( {q,a}\right) \sim \mathcal{D},{\left\{ {o}_{i}\right\} }_{i = 1}^{G} \sim {\pi }_{{\theta }_{\text{old }}}\left( {\cdot \mid q}\right) } \left\lbrack {\frac{1}{G}\mathop{\sum }\limits_{{i = 1}}^{G}\frac{1}{\left| {o}_{i}\right| }\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }\left( {\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},\operatorname{clip}\left( {{r}_{i,t}\left( \theta \right) ,1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{i,t}}\right)
-\beta {D}_{\mathrm{{KL}}}\left( {{\pi }_{\theta }\parallel {\pi }_{\mathrm{{ref}}}}\right) }\right) }\right\rbrack,\\
\text{where }r_{i,t}(\theta) = \dfrac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q,o_{i,<t})}.
\end{gathered}
$$

> -   $\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},\operatorname{clip}\left( {{r}_{i,t}\left( \theta \right) ,1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{i,t}}\right)$ï¼š**è£å‰ªæ›¿ä»£ç›®æ ‡å‡½æ•°(clipped surrogated objective)**ï¼Œç±»ä¼¼äº PPO ä¸­çš„ clipped objectiveï¼Œé™åˆ¶äº†é‡è¦æ€§é‡‡æ ·æ¯”ç‡ $r_{i,t}(\theta)$ çš„å½±å“èŒƒå›´åœ¨ $[1-\epsilon,1+\epsilon]$ ä¸­ã€‚
> -   $-\beta {D}_{\mathrm{{KL}}}\left( {{\pi }_{\theta }\parallel {\pi }_{\mathrm{{ref}}}}\right)$ï¼š**KL æƒ©ç½šé¡¹(KL penalty term)** ç”¨äºçº¦æŸæ–°ç­–ç•¥ $\pi_{\theta}$ å’Œ **å‚è€ƒç­–ç•¥(reference policy)** $\pi_{\text{ref}}$ï¼ˆé€šå¸¸æ˜¯åˆå§‹ç­–ç•¥æˆ–æ—§ç­–ç•¥ï¼‰ä¹‹é—´çš„ KL æ•£åº¦ã€‚æœ¬è®ºæ–‡ï¼ˆGAPOï¼‰==ç§»é™¤äº† KL æƒ©ç½šé¡¹==ï¼Œä»¥èµ‹äºˆæ¨¡å‹æ›´å¤§çš„æ¢ç´¢ç©ºé—´ã€‚

### 1.3. Rule-based Reward Modeling

ä¸ºäº†é¿å… **å¥–åŠ±ä½œå¼Šé—®é¢˜(reward hacking problem)**ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å¯éªŒè¯ä»»åŠ¡çš„æœ€ç»ˆå‡†ç¡®ç‡ä½œä¸ºç»“æœå¥–åŠ±ï¼Œä½¿ç”¨ä»¥ä¸‹è§„åˆ™è®¡ç®—ï¼š

$$
R({\widehat{y},y}) = \begin{cases} 1, & \texttt{is\_equivalent}({\widehat{y},y}) \\ - 1, & \text{ otherwise } \end{cases}
$$

> -   $\texttt{is\_equivalent}({\widehat{y},y})$ è¡¨ç¤ºåˆ¤æ–­é¢„æµ‹ç­”æ¡ˆ $\hat{y}$ ä¸æ ‡å‡†ç­”æ¡ˆ $y$ æ˜¯å¦ç­‰ä»·ã€‚

è¿™ä¸€æ–¹æ³•è¢«è¯æ˜åœ¨ **è‡ªåŠ¨å®šç†è¯æ˜(automated theorem proving)**ã€æ•°å­¦ç«èµ›ã€è®¡ç®—æœºç¼–ç¨‹ç­‰é¢†åŸŸèƒ½å¤Ÿæœ‰æ•ˆæ¿€æ´»åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## 2. Method

### 2.1. Objective Function

æœ¬è®ºæ–‡æå‡º DAPO ç®—æ³•ï¼Œåœ¨ GRPO åŸºç¡€ä¸Šé€šè¿‡ä»¥ä¸‹ç›®æ ‡å‡½æ•°ä¼˜åŒ–ï¼š

$$
\begin{gathered}
{\mathcal{J}}_{\text{DAPO}}\left( \theta \right) = {\mathbb{E}}_{\left( {q,a}\right) \sim \mathcal{D},{\left\{ {o}_{i}\right\} }_{i = 1}^{G} \sim {\pi }_{{\theta }_{\text{old }}}\left( {\cdot \mid q}\right) } \left\lbrack {
{\color{teal}\frac{1}{\mathop{\sum }_{{i = 1}}^{G}\left| {o}_{i}\right| }\mathop{\sum }\limits_{{i = 1}}^{G}\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }}
\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},
{\color{brown}\operatorname{clip}\left( { {\color{black}{r}_{i,t}\left( \theta \right)} ,1 - {\varepsilon }_{\text{low }},1 + {\varepsilon }_{\text{high }}}\right)}
{\widehat{A}}_{i,t}}\right) }\right\rbrack, \\
\text{s.t. }\color{purple}0 <\left|\left\{{{o}_{i}\mid\texttt{is\_equivalent}({a,{o}_{i}})}\right\}\right|< G,\\
\text{where }{r}_{i,t}\left( \theta \right) = \frac{{\pi }_{\theta }\left( {{o}_{i,t} \mid q,{o}_{i, < t}}\right) }{{\pi }_{{\theta }_{\text{old }}}\left( {{o}_{i,t} \mid q,{o}_{i, < t}}\right) },\;{\widehat{A}}_{i,t} = \frac{{R}_{i} - \operatorname{mean}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }{\operatorname{std}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }.
\end{gathered}
$$

### 2.2. Clip-Higher

**Motivation**ï¼šä½œè€…å‘ç°ï¼Œåœ¨æœ´ç´ çš„ PPO æˆ– GRPO è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œç­–ç•¥çš„ç†µè¿…é€Ÿä¸‹é™(ç§°ä¸º **ç†µå´©æºƒ)** ï¼‰ï¼Œæ¨¡å‹ä¸§å¤±äº†æ¢ç´¢æ–°è¡Œä¸ºçš„èƒ½åŠ›ã€‚ä½œè€…å‘ç°ï¼Œä¸Šé™è£å‰ªåœ¨ç­–ç•¥æ¢ç´¢æ–¹é¢èµ·åˆ°äº†å‰¯ä½œç”¨ï¼š

> [!example] Example: ä¸Šé™è£å‰ªé™åˆ¶ä½æ¦‚ç‡ token çš„æ¦‚ç‡æå‡
>
> å‡è®¾è£å‰ªå‚æ•° $\varepsilon = 0.2$ï¼Œè€ƒè™‘ä¸¤ä¸ªåŠ¨ä½œï¼Œå®ƒä»¬çš„æ—§ç­–ç•¥æ¦‚ç‡ ${\pi }_{{\theta }_{\text{old }}}\left( {{o}_{i} \mid q}\right)$ åˆ†åˆ«ä¸º 0.01 å’Œ 0.9ã€‚
>
> -   å¯¹äºæ¦‚ç‡ä¸º 0.01 çš„åŠ¨ä½œï¼Œç»è¿‡ä¸Šé™è£å‰ªåï¼Œå…¶æ¦‚ç‡ ${\pi }_{\theta }\left( {{o}_{i} \mid q}\right)$ æœ€å¤§åªèƒ½æ›´æ–°åˆ° $0.01 \times (1 + 0.2) = 0.012$ã€‚
> -   å¯¹äºæ¦‚ç‡ä¸º 0.9 çš„åŠ¨ä½œï¼Œç»è¿‡ä¸Šé™è£å‰ªåï¼Œå…¶æ¦‚ç‡ ${\pi }_{\theta }\left( {{o}_{i} \mid q}\right)$ æœ€å¤§å¯ä»¥æ›´æ–°åˆ° $0.9 \times (1 + 0.2) = 1.08$ï¼Œä½†å®é™…æ¦‚ç‡å€¼ä¼šè¢«é™åˆ¶åœ¨ 1 ä»¥å†…ï¼Œå› æ­¤æœ€å¤§æ›´æ–°åˆ° 1ã€‚

**Solution**ï¼šä½œè€…æå‡ºå¯ä»¥ ==è§£è€¦è£å‰ªèŒƒå›´çš„ä¸Šä¸‹ç•Œ==ï¼Œå³ï¼šå°†è£å‰ªæ“ä½œ $\text{clip}(\cdot,1-\epsilon,1+\epsilon)$ æ›¿æ¢ä¸º $\color{brown}{\text{clip}(\cdot,1-\epsilon_{\text{low}},1+\epsilon_{\text{high}})}$ï¼Œä»è€Œç»™ä½æ¦‚ç‡ token ç•™å‡ºæ›´å¤§çš„æå‡ç©ºé—´ï¼Œæœ‰åŠ©äºæé«˜ç­–ç•¥ç†µã€‚

![|800](https://img.memset0.cn/2025/03/22/Bv9TmKCa.png)

### 2.3. Dynamic Sampling

**Motivation**ï¼šåœ¨ç°æœ‰çš„ PPO æˆ– GRPO çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚æœæ¨¡å‹åœ¨æŸäº›ç‰¹å®š prompt ä¸Šè¡¨ç°ç‰¹åˆ«å¥½ï¼Œç”Ÿæˆçš„æ‰€æœ‰å›å¤éƒ½æ˜¯æ­£ç¡®æ—¶ï¼Œå°±ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œå½±å“è®­ç»ƒã€‚

**Solution**ï¼šä½œè€…æå‡º **åŠ¨æ€é‡‡æ ·(dynamic sampling)** ç­–ç•¥ï¼Œæ ¸å¿ƒåœ¨äº **è¿‡é‡‡æ ·(oversampling)** å’Œ **è¿‡æ»¤(filtering)**ã€‚é€šè¿‡é¢å¤–é‡‡æ ·å¹¶ç”¨çº¦æŸæ¡ä»¶ $\color{purple}0 <\left|\left\{{{o}_{i}\mid\texttt{is\_equivalent}({a,{o}_{i}})}\right\}\right|< G$ è¿‡æ»¤æ‰å›å¤å…¨éƒ¨æ­£ç¡®æˆ–å…¨éƒ¨é”™è¯¯çš„æ ·æœ¬ï¼Œä»è€Œè®©æ¯ä¸ª batch éƒ½å……æ»¡æ¢¯åº¦ä¸ä¸º $0$ çš„æ ·æœ¬ã€‚

ä½œè€…è¿˜æŒ‡å‡ºï¼ŒDynamic Sampling ç­–ç•¥ä¸ä¸€å®šä¼šé™ä½è®­ç»ƒæ•ˆç‡ã€‚å› ä¸ºåœ¨ä¸é‡‡ç”¨æµæ°´çº¿æŠ€æœ¯æˆ–å¹¶è¡ŒæŠ€æœ¯çš„é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€ batch çš„ç”Ÿæˆæ—¶é—´é€šå¸¸æ˜¯ç”±ç”Ÿæˆæ—¶é—´å¼‚å¸¸é•¿çš„â€œ**é•¿å°¾æ ·æœ¬(long-tail sample)**â€æ‰€ä¸»å¯¼çš„ï¼Œè€Œâ€œè¿‡é‡‡æ ·â€æ­¥éª¤å¤§æ¦‚ç‡åªä¼šå¢åŠ è½»é‡æ ·æœ¬çš„é‡‡æ ·ï¼Œè€Œä¸ä¼šæ˜æ˜¾å¢åŠ é‡‡æ ·æ—¶é—´ã€‚

![|800](https://img.memset0.cn/2025/03/22/eu9Lf2xV.png)

### 2.4. Token-Level Policy Gradient Loss

**Motivation**ï¼šåœ¨ GPRO ä¸­æˆ‘ä»¬é‡‡ç”¨äº† sample-level çš„æŸå¤±å¹³å‡ $\displaystyle{\dfrac{1}{G}\sum_{i=1}^{G} \dfrac{1}{|o_{i}|}\sum_{i=1}^{|o_{i}|}}$ï¼Œè¿™å°†å¯¼è‡´å¯¹äºé•¿æ ·æœ¬ï¼Œæ¯ä¸ª token å¯¹æ€»ä½“çš„æŸå¤±ä¼šè¢«ç¨€é‡Šã€‚è€Œä½œè€…å‘ç°ï¼Œ==è¿‡é•¿çš„æ ·æœ¬å°å°ä¼šå‡ºç°ä½è´¨é‡æ¨¡å¼==ï¼Œå¦‚èƒ¡è¨€ä¹±è¯­å’Œé‡å¤è¯æ±‡ï¼Œä½œè€…å¸Œæœ›èƒ½æœ‰æ•ˆæƒ©ç½šè¿™äº›ä¸è‰¯æ¨¡å¼ã€‚

**Solution**ï¼šåœ¨æœ¬è®ºæ–‡ä¸­ä½œè€…é‡‡ç”¨ token-level çš„æŸå¤±å¹³å‡ $\displaystyle{{\color{teal}\frac{1}{\mathop{\sum }_{{i = 1}}^{G}\left| {o}_{i}\right| }\mathop{\sum }\limits_{{i = 1}}^{G}\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }}}$ï¼Œè¿™æ ·é•¿æ ·æœ¬å¯¹æ€»ä½“æ¢¯åº¦æ›´æ–°çš„å½±å“æ›´å¤§ã€‚å¦å¤–ä½œè€…æŒ‡å‡ºï¼Œå¯¹äºå•ä¸ªè¯å…ƒï¼Œå¦‚æœæŸç§ç‰¹å®šçš„ç”Ÿæˆæ¨¡å¼èƒ½å¯¼è‡´å¥–åŠ±å¢åŠ æˆ–å‡å°‘ï¼Œæ— è®ºå®ƒå‡ºç°åœ¨å¤šé•¿çš„å“åº”ä¸­ï¼Œéƒ½ä¼šå—åˆ°åŒç­‰ç¨‹åº¦çš„æ¿€åŠ±æˆ–æŠ‘åˆ¶ã€‚

### 2.5. Soft Overlong Punishment

**Motivation**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯èƒ½è®¾æˆè¿‡é•¿çš„æ–‡æœ¬è€Œéœ€è¦è¿›è¡Œæˆªæ–­å¤„ç†ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¸¸è§çš„åšæ³•æ˜¯ç»™è¢«æˆªæ–­çš„æ ·æœ¬åˆ†é…ä¸€ä¸ª**æƒ©ç½šæ€§å¥–åŠ±(punitive reward)**ã€‚ç„¶è€Œï¼Œä½œè€…å‘ç°è¿™ç§æƒ©ç½šä¼šå¸¦æ¥ **å¥–åŠ±å™ªå£°(reward noise)**ï¼Œå³ä¸€ä¸ªä¼˜ç§€çš„æ¨ç†è¿‡ç¨‹å¯èƒ½å› ä¸ºå…¶ç”Ÿæˆäº†è¿‡é•¿çš„æ–‡æœ¬è€Œè¢«æƒ©ç½šã€‚

**Solution**ï¼šä½œè€…é‡‡å–äº†ä¸€ç§ **è½¯æ€§è¿‡é•¿æƒ©ç½š(soft overlong punishment)** çš„ç­–ç•¥ï¼Œè®¾ç½®æœ€å¤§ç”Ÿæˆé•¿åº¦ $L_{\text{max}}$ ä¸â€œè½¯æƒ©ç½šç¼“å†²é•¿åº¦â€ $L_{\text{cache}}$ ä»è€Œå¹³è¡¡æƒ©ç½šæ€§æŸå¤±å’Œç›´æ¥å±è”½æŸå¤±çš„æ–¹æ³•ï¼š

$$
{R}_{\text{length }}\left( y\right) = \left\{ \begin{array}{ll} 0, & \left| y\right| \leq {L}_{\max } - {L}_{\text{cache }} \\ \frac{\left( {{L}_{\max } - {L}_{\text{cache }}}\right) - \left| y\right| }{{L}_{\text{cache }}}, & {L}_{\max } - {L}_{\text{cache }} < \left| y\right| \leq {L}_{\max } \\ - 1, & {L}_{\max } < \left| y\right| \end{array}\right.
$$

## 3. Experiments

### 3.1. Dataset Transformation

å¯¹äºæ•°å­¦é¢˜ç›®æ•°æ®é›†ï¼Œç”±äºç­”æ¡ˆæ ¼å¼å¤šç§å¤šæ ·ï¼Œä¸ºäº†æ–¹ä¾¿å®ç°ç­‰ä»·æ€§åˆ¤æ–­å‡½æ•° $\texttt{is\_equivalent}(\hat{y},y)$ çš„å®ç°ï¼Œä½œè€…å—åˆ°ç¾å›½æ•°å­¦ç«èµ› AIME çš„å¯å‘ï¼Œå°†ç­”æ¡ˆå…¨éƒ¨è½¬åŒ–ä¸ºæ•´æ•°æ¥è§£æå¹¶æ¯”è¾ƒï¼Œä»è€Œå¾—åˆ°äº† DAPO-Math-17K æ•°æ®é›†ã€‚

> [!example]- å°†ç­”æ¡ˆè½¬åŒ–ä¸ºæ•´æ•°çš„ä¾‹å­
>
> **åŸå§‹é—®é¢˜(Original Problem)**ï¼š
>
> Let $x$ and $y$ be real numbers such that ${x}^{2} + {y}^{2} - {22x} - {16y} + {113} = 0$ . Determine the smallest possible value of $x$ .
>
> Answer: ${11} - 2\sqrt{6}$
>
> **è½¬æ¢åçš„é—®é¢˜(Transformed Problem)**ï¼š
>
> Let $x$ and $y$ be real numbers such that ${x}^{2} + {y}^{2} - {22x} - {16y} + {113} = 0$ . Determine the smallest possible value of $x$ . The original answer is in the form $k - m\sqrt{n}$ ,where $k,m$ ,and $n$ are integers. Please find the value of $k + m + n$ .
>
> Answer: 19






