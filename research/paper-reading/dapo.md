---
# 
title: "ã€Œè®ºæ–‡ç²¾è¯» #21ã€DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
create-date: 2025-03-21 11:34:22
update-date: 2025-03-21 11:34:22
link-chat: https://chat.memset0.cn/chat?session=ssn_uaicSGbAMyYz&topic=tpc_UKZ97tf6nrWq
slug: /research/paper-reading/dapo
# indexed: true
tags:
  - Reinforcement-Learning
  - PPO
  - GRPO
  - DAPO
  - topic/web-agent
# 
citekey: yuDAPOOpenSourceLLM2025
doi: "10.48550/arXiv.2503.14476" 
export-date: 2025-03-21 19:50:36
---



## 1. Background

### 1.1. Proximal Policy Optimization (PPO)

> -   ğŸ‘ ä»·å€¼å‡½æ•° $V(s)$ éœ€è¦ä¸€ä¸ªé¢å¤–çš„ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸ç­–ç•¥ç½‘ç»œä¸€èµ·è®­ç»ƒã€‚

**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization, PPO)** æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºè®­ç»ƒæ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

PPO çš„ç›®æ ‡å‡½æ•°æ˜¯æœ€å¤§åŒ–ï¼š

$$
{\mathcal{J}}_{\mathrm{{PPO}}}\left( \theta \right) = {\mathbb{E}}_{\left( {q,a}\right) \sim \mathcal{D},{o}_{ \leq t} \sim {\pi }_{{\theta }_{\text{old }}}\left( {\cdot \mid q}\right) }\left\lbrack {\min \left( {\frac{{\pi }_{\theta }\left( {{o}_{t} \mid q,{o}_{ < t}}\right) }{{\pi }_{{\theta }_{\text{old }}}\left( {{o}_{t} \mid q,{o}_{ < t}}\right) }{\widehat{A}}_{t},\operatorname{clip}\left( {\frac{{\pi }_{\theta }\left( {{o}_{t} \mid q,{o}_{ < t}}\right) }{{\pi }_{{\theta }_{\text{old }}}\left( {{o}_{t} \mid q,{o}_{ < t}}\right) },1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{t}}\right) }\right\rbrack
$$

> -   $(q,a)\sim \mathcal{D}$ï¼šä» data åˆ†å¸ƒä¸­éšæœºé‡‡æ ·çš„ question-answer å¯¹ã€‚
> -   $o_{\leq t} \sim \pi_{\theta_{\text{old}}} (\cdot|q)$ï¼šä½¿ç”¨æ—§ç­–ç•¥ï¼Œæ ¹æ®é—®é¢˜ $q$ ç”Ÿæˆå‰ $t$ æ­¥çš„ tokensã€‚
> -   $\pi_{\theta}(o_{t}|q,o_{<t})$ï¼šåœ¨ç»™å®š $q$ å’Œå‰ $t-1$ æ­¥æ“ä½œçš„ tokens $o_{<t}$ çš„æƒ…å†µä¸‹ï¼Œå½“å‰ç­–ç•¥ $\pi_{\theta}$ è‰²èŠ±å§‘å¨˜æˆ token $o_{t}$ çš„æ¦‚ç‡ã€‚
> -   $\displaystyle{\dfrac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{\text{old}}}(o_{t}|q,o_{<t})}}$ï¼šæ–°æ—§ç­–ç•¥ç”Ÿæˆ token $o_{t}$ çš„æ¦‚ç‡æ¯”ä¾‹ï¼Œç§°ä¸º **é‡è¦æ€§é‡‡æ ·æ¯”ç‡(importance sampling ratio)**ï¼Œè¡¡é‡äº†æ–°æ—§ç­–ç•¥çš„å·®å¼‚ã€‚
> -   $\hat{A}_{t}$ï¼š**ä¼˜åŠ¿å‡½æ•°(advantage function)** çš„ä¼°è®¡å€¼ã€‚
> -   $\text{clip}\left(\dfrac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}, 1 - \varepsilon, 1 + \varepsilon\right)$ï¼š**è£å‰ª(clip)** æ˜¯ PPO çš„æ ¸å¿ƒæ€æƒ³ï¼Œå½“é‡è¦é‡‡æ ·æ€§æ¯”ç‡è¶…è¿‡ $[1-\epsilon,1+\epsilon]$ çš„èŒƒå›´å†…æ—¶ï¼Œ==å°†å…¶è£å‰ªåˆ°è¿™ä¸ªèŒƒå›´å†…==ã€‚

å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸Šå‡ç®—æ³•é€æ­¥æ›´æ–°ç­–ç•¥å‚æ•° $\theta$ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼Œæˆ‘ä»¬è®¡ç®— (1) é‡è¦æ€§é‡‡æ ·ç‡ $\times$ $\hat{A}_{t}$ (2) è£å‰ªåçš„é‡è¦æ€§é‡‡æ ·ç‡ $\times$ $\hat{A}_{t}$ ä¸­çš„æœ€å°å€¼ä½œä¸ºæœ€å¤§åŒ–ç›®æ ‡ï¼Œä»è€Œé€æ­¥æå‡ç­–ç•¥çš„æ€§èƒ½ã€‚è¿™é‡Œè£å‰ª+æœ€å°å€¼çš„ä½œç”¨ä¸»è¦æ˜¯==ç¡®ä¿ç­–ç•¥æ›´æ–°èƒ½å¤Ÿæå‡å¥–åŠ±ï¼Œä½†åˆä¸ä¼šåç¦»æ—§ç­–ç•¥å¤ªè¿œ==ã€‚

å…¶ä¸­ï¼Œä¼˜åŠ¿å‡½æ•°ä¼°è®¡ $\hat{A}_{t}$ é€šè¿‡ **å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(Generalized Advantage Estimation, GAE)** æ¥è®¡ç®—ï¼š

$$
{\widehat{A}}_{t}^{\mathrm{{GAE}}\left( {\gamma ,\lambda }\right) } = \mathop{\sum }\limits_{{l = 0}}^{\infty }{\left( \gamma \lambda \right) }^{l}{\delta }_{t + l},\quad 0\leq \gamma,\lambda\leq1.
$$

> -   $\lambda$ï¼šGAE å‚æ•°ï¼Œè¶…å‚æ•°ï¼Œæ§åˆ¶åå·®-æ–¹å·®å‡è¡¡ã€‚

å…¶ä¸­æ—¶é—´æ­¥ $l$ çš„ **æ—¶åºå·®åˆ†è¯¯å·®(temporal difference error, TD error)** $\delta_{l}$ çš„è®¡ç®—æ–¹å¼ä¸ºï¼š

$$
{\delta }_{l} = {R}_{l} + {\gamma V}\left( {s}_{l + 1}\right) - V\left( {s}_{l}\right)
$$

> -   $R_{l} +\gamma V(s_{l+1})$ï¼š**æ—¶åºå·®åˆ†ç›®æ ‡(temporal difference targetï¼ŒTD target)**ï¼Œå³æ—¶å¥–åŠ±ä¸æŠ˜æ‰£åçš„æœªæ¥çŠ¶æ€ä»·å€¼ã€‚
> -   $V(s_{l})$ï¼šå½“å‰çŠ¶æ€ä»·å€¼ã€‚
> -   è¿™ç§å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•æˆä¸º **æ—¶åºå·®åˆ†å­¦ä¹ (temporal difference learning, TD learning)**ï¼Œå…¶è¿ç”¨äº† **è‡ªä¸¾(bootstrapping)** çš„æ€æƒ³ï¼Œä¸å¿…ä»¥æ¥çœŸå®ä»·å€¼è€Œæ˜¯é€šè¿‡ä»·å€¼å‡½æ•°è‡ªèº«è¿›è¡Œè®­ç»ƒã€‚

> [!note] ä»è´å°”æ›¼æœŸæœ›æ–¹ç¨‹çš„è§’åº¦ç†è§£æ—¶åºå·®åˆ†è¯¯å·®
>
> è¿™ä¸€ç›®æ ‡å‡½æ•°æ˜¯é€šè¿‡ **è´å°”æ›¼æœŸæœ›æ–¹ç¨‹(Bellman expectation equation)** å¾—åˆ°çš„ï¼Œç”¨ä»¥==æè¿°ä»·å€¼å‡½æ•° $V(s)$ åº”è¯¥æ»¡è¶³çš„å…³ç³»==ã€‚è´å°”æ›¼æœŸæœ›æ–¹ç¨‹å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š
>
> $$
> V(S_{t}) = E_{\pi} \left[ R_{t+1} + \gamma V(S_{t+1}) \right]
> $$
>
> å…¶ä¸­ï¼Œ$R+V(s)$ çš„éƒ¨åˆ†å³ä¸Šæ–‡æåˆ°çš„ TD target æˆ–ç§° **è´å°”æ›¼å¤‡ä»½(Bellman backup)**ã€‚è¿™ä¸ªæ–¹ç¨‹è¡¨è¾¾çš„æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡ä»·å€¼å‡½æ•°çš„ç›®æ ‡æ˜¯ï¼šâ€œçŠ¶æ€ $s$ çš„ä»·å€¼ç­‰äºåœ¨çŠ¶æ€ $s$ æ—¶ï¼Œé‡‡å–ç­–ç•¥ $\pi$ åï¼Œè·å¾—çš„å³æ—¶å¥–åŠ± $R_{t+1}$ åŠ ä¸ŠæŠ˜æ‰£å› å­ $\gamma$ ä¹˜ä»¥è½¬ç§»åˆ°çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ $S_{t+1}$ çš„ä»·å€¼ $V(S_{t+1})$ çš„æœŸæœ›â€ã€‚
>
> ä¼˜åŒ– TD error çš„ç›®æ ‡å³é€šè¿‡è®¾å®šçš„å¥–åŠ± $R$ ä¸æ–­è°ƒæ•´ä»·å€¼å‡½æ•° $V(s)$ çš„ä¼°è®¡ï¼Œä½¿ TD error è¶‹è¿‘äº $0$ï¼Œå³è®©ä»·å€¼å‡½æ•°å°½å¯èƒ½åœ°æ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ã€‚æ¢å¥è¯è¯´ï¼ŒTD error æœ¬è´¨ä¸Šå°±æ˜¯ä»·å€¼å‡½æ•°éœ€è¦è¿›è¡Œçš„è°ƒæ•´ã€‚
>
> å½“ç„¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¡ç®—è¿™ä¸€æœŸæœ›ï¼Œåªèƒ½é€šè¿‡ä¸ç¯å¢ƒäº¤äº’é‡‡æ ·æ¥å¹¶ä¸æ–­æ›´æ–°ä»·å€¼å‡½æ•°ã€‚
>
> > [!example]- Example by Gemini 2.0
> >
> > **ä¸€ä¸ªç®€å•çš„ç±»æ¯”**: æ¯”å¦‚ä½ æ­£åœ¨å­¦ä¹ ä¼°è®¡ä¸€ä¸ªæˆ¿å­çš„ä»·å€¼ã€‚
> >
> > -   **$V(s_l)$**: æ˜¯ä½  **å½“å‰å¯¹è¿™ä¸ªæˆ¿å­çš„ä¼°ä»·**ï¼Œ æ¯”å¦‚ä½ ç°åœ¨ä¼°è®¡è¿™ä¸ªæˆ¿å­å€¼ 100 ä¸‡ã€‚ ($s_l$ å¯ä»¥ç†è§£ä¸ºæè¿°æˆ¿å­å½“å‰çŠ¶æ€çš„ä¿¡æ¯ï¼Œæ¯”å¦‚åœ°æ®µã€é¢ç§¯ã€æˆ¿é¾„ç­‰)ã€‚
> > -   **${R}_{l}$**: å¯ä»¥ç†è§£ä¸ºæˆ¿å­ **å‡ºç§Ÿè·å¾—çš„ç§Ÿé‡‘**ï¼Œ æ¯”å¦‚ä½ æŠŠæˆ¿å­å‡ºç§Ÿå‡ºå»ï¼Œè·å¾—äº† 1 ä¸‡çš„ç§Ÿé‡‘ã€‚
> > -   **$V(s_{l + 1})$**: æ˜¯ä½  **å¯¹æˆ¿å­æœªæ¥ä»·å€¼çš„é‡æ–°ä¼°è®¡**ï¼Œ æ¯”å¦‚è€ƒè™‘åˆ°æˆ¿ä»·ä¸Šæ¶¨çš„è¶‹åŠ¿ï¼Œä½ æŠŠå¯¹æˆ¿å­æœªæ¥ä»·å€¼çš„ä¼°è®¡æé«˜åˆ°äº† 105 ä¸‡ã€‚ ($s_{l+1}$ å¯ä»¥ç†è§£ä¸ºæˆ¿å­çŠ¶æ€åœ¨æ—¶é—´æ¨ç§»åå‘ç”Ÿå˜åŒ–ï¼Œä¾‹å¦‚æˆ¿é¾„å¢åŠ ï¼Œä½†å‘¨è¾¹é…å¥—è®¾æ–½æ›´å®Œå–„ç­‰)ã€‚
> > -   **${\gamma}$**: æŠ˜æ‰£å› å­ï¼Œ æ¯”å¦‚ 0.9ï¼Œ è¡¨ç¤ºä½ å¯¹æœªæ¥ä»·å€¼çš„é‡è§†ç¨‹åº¦ã€‚
> >
> > é‚£ä¹ˆï¼ŒTD ç›®æ ‡å°±æ˜¯ ${R}_{l} + {\gamma V}\left( {s}_{l + 1}\right) = 1 \text{ä¸‡} + 0.9 \times 105 \text{ä¸‡} = 95.5 \text{ä¸‡}$ã€‚ TD è¯¯å·®å°±æ˜¯ ${\delta }_{l} = 95.5 \text{ä¸‡} - 100 \text{ä¸‡} = -4.5 \text{ä¸‡}$ã€‚
> >
> > ç”±äº TD è¯¯å·®æ˜¯è´Ÿçš„ï¼Œè¯´æ˜ä½  **ä¹‹å‰çš„ä¼°ä»· 100 ä¸‡ å¯èƒ½åé«˜äº†**ã€‚ TD è¯¯å·®å‘Šè¯‰ä½ ï¼Œä½ åº”è¯¥æŠŠå¯¹æˆ¿å­çš„ä¼°ä»· **é™ä½** ä¸€äº›ï¼Œæ›´æ¥è¿‘ 95.5 ä¸‡è¿™ä¸ª â€œç›®æ ‡å€¼â€ã€‚ é€šè¿‡ä¸æ–­åœ°æ ¹æ®ç§Ÿé‡‘æ”¶å…¥å’Œæœªæ¥æˆ¿ä»·é¢„æœŸæ¥è°ƒæ•´ä½ çš„æˆ¿å­ä¼°ä»·ï¼Œä½ å°±èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡æˆ¿å­çš„ä»·å€¼ã€‚

### 1.2. Group Relative Policy Optimizati(GRPO)

- [ã€Œè®ºæ–‡ç²¾è¯» #22ã€DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](/research/paper-reading/deepseek-math/)

> -   ğŸ‘ åˆ©ç”¨åŒä¸€é—®é¢˜ä¸‹ä¸åŒå›ç­”çš„ç›¸å¯¹å¥½åç¨‹åº¦æ¥æŒ‡å¯¼ç­–ç•¥çš„æ›´æ–°ï¼Œæ›´é€‚ç”¨äºå¥–åŠ±ä¿¡å·ç¨€ç–æˆ–éš¾ä»¥è®¾è®¡çš„åœºæ™¯ã€‚

**ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)** çš„æ ¸å¿ƒæ€æƒ³åœ¨äºæ‘’å¼ƒäº† PPO çš„ä»·å€¼å‡½æ•°è®¾è®¡ï¼Œè½¬è€Œ==åˆ©ç”¨åŒä¸€é—®é¢˜ä¸‹ä¸åŒå›ç­”ä¹‹é—´çš„ç›¸å¯¹å¥½åç¨‹åº¦==æ¥ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ã€‚

å¯¹äºç»™å®šçš„ question-answer å¯¹ $(q,a)$ï¼Œæ—§è¡Œä¸ºç­–ç•¥ $\pi_{\theta_{\text{old}}}$ ä¼šå¯¹è¿™ä¸€é—®é¢˜é‡‡æ ·ä¸€ **ç»„(group)** $G$ ä¸ªä¸åŒçš„å›ç­” $\{ o_{i} \}_{i=1}^{G}$ã€‚æ¥ç€ï¼ŒGRPO ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—ç¬¬ $i$ ä¸ªå›ç­”çš„ä¼˜åŠ¿å‡½æ•°ï¼š

$$
{\widehat{A}}_{i,t} = \frac{{R}_{i} - \operatorname{mean}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }{\operatorname{std}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }
$$

> -   $\text{mean}(\cdot)$ è¡¨ç¤ºå¹³å‡å€¼ï¼Œ$\text{std}(\cdot)$ è¡¨ç¤ºæ ‡å‡†å·®ã€‚
> -   ä¼˜åŠ¿å‡½æ•° $\widehat{A}_{i,t}$ å¯ä»¥çœ‹åšå¯¹ç›´æ¥å¥–åŠ±çš„ **æ ‡å‡†åŒ–(standardization)**ï¼Œé€šè¿‡å°†å‡å€¼å¹³ç§»åˆ° $0$ï¼Œæ ‡å‡†å·®ç¼©æ”¾åˆ° $1$ã€‚
> -   ç‰¹åˆ«åœ°ï¼Œæœ¬ç®—æ³•ï¼ˆDAPOï¼‰ä¼šç›´æ¥è¿‡æ»¤æ‰æ‰€æœ‰å›ç­”å¥–åŠ±ç›¸åŒçš„æƒ…å†µï¼Œä»¥é¿å…é™¤ä»¥ $0$ çš„é—®é¢˜ã€‚

$$
\begin{gathered}
{\mathcal{J}}_{\text{GRPO }}\left( \theta \right) = {\mathbb{E}}_{\left( {q,a}\right) \sim \mathcal{D},{\left\{ {o}_{i}\right\} }_{i = 1}^{G} \sim {\pi }_{{\theta }_{\text{old }}}\left( {\cdot \mid q}\right) } \left\lbrack {\frac{1}{G}\mathop{\sum }\limits_{{i = 1}}^{G}\frac{1}{\left| {o}_{i}\right| }\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }\left( {\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},\operatorname{clip}\left( {{r}_{i,t}\left( \theta \right) ,1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{i,t}}\right)
-\beta {D}_{\mathrm{{KL}}}\left( {{\pi }_{\theta }\parallel {\pi }_{\mathrm{{ref}}}}\right) }\right) }\right\rbrack,\\
\text{where }r_{i,t}(\theta) = \dfrac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q,o_{i,<t})}.
\end{gathered}
$$

> -   $\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},\operatorname{clip}\left( {{r}_{i,t}\left( \theta \right) ,1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{i,t}}\right)$ï¼š**è£å‰ªæ›¿ä»£ç›®æ ‡å‡½æ•°(clipped surrogated objective)**ï¼Œç±»ä¼¼äº PPO ä¸­çš„ clipped objectiveï¼Œé™åˆ¶äº†é‡è¦æ€§é‡‡æ ·æ¯”ç‡ $r_{i,t}(\theta)$ çš„å½±å“èŒƒå›´åœ¨ $[1-\epsilon,1+\epsilon]$ ä¸­ã€‚
> -   $-\beta {D}_{\mathrm{{KL}}}\left( {{\pi }_{\theta }\parallel {\pi }_{\mathrm{{ref}}}}\right)$ï¼š**KL æƒ©ç½šé¡¹(KL penalty term)** ç”¨äºçº¦æŸæ–°ç­–ç•¥ $\pi_{\theta}$ å’Œ **å‚è€ƒç­–ç•¥(reference policy)** $\pi_{\text{ref}}$ï¼ˆé€šå¸¸æ˜¯åˆå§‹ç­–ç•¥æˆ–æ—§ç­–ç•¥ï¼‰ä¹‹é—´çš„ KL æ•£åº¦ã€‚æœ¬è®ºæ–‡ï¼ˆGAPOï¼‰==ç§»é™¤äº† KL æƒ©ç½šé¡¹==ï¼Œä»¥èµ‹äºˆæ¨¡å‹æ›´å¤§çš„æ¢ç´¢ç©ºé—´ã€‚

### 1.3. Rule-based Reward Modeling

ä¸ºäº†é¿å… **å¥–åŠ±ä½œå¼Šé—®é¢˜(reward hacking problem)**ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å¯éªŒè¯ä»»åŠ¡çš„æœ€ç»ˆå‡†ç¡®ç‡ä½œä¸ºç»“æœå¥–åŠ±ï¼Œä½¿ç”¨ä»¥ä¸‹è§„åˆ™è®¡ç®—ï¼š

$$
R({\widehat{y},y}) = \begin{cases} 1, & \texttt{is\_equivalent}({\widehat{y},y}) \\ - 1, & \text{ otherwise } \end{cases}
$$

> -   $\texttt{is\_equivalent}({\widehat{y},y})$ è¡¨ç¤ºåˆ¤æ–­é¢„æµ‹ç­”æ¡ˆ $\hat{y}$ ä¸æ ‡å‡†ç­”æ¡ˆ $y$ æ˜¯å¦ç­‰ä»·ã€‚

è¿™ä¸€æ–¹æ³•è¢«è¯æ˜åœ¨ **è‡ªåŠ¨å®šç†è¯æ˜(automated theorem proving)**ã€æ•°å­¦ç«èµ›ã€è®¡ç®—æœºç¼–ç¨‹ç­‰é¢†åŸŸèƒ½å¤Ÿæœ‰æ•ˆæ¿€æ´»åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## 2. Method

### 2.1. Objective Function

æœ¬è®ºæ–‡æå‡º DAPO ç®—æ³•ï¼Œåœ¨ GRPO åŸºç¡€ä¸Šé€šè¿‡ä»¥ä¸‹ç›®æ ‡å‡½æ•°ä¼˜åŒ–ï¼š

$$
\begin{gathered}
{\mathcal{J}}_{\text{DAPO}}\left( \theta \right) = {\mathbb{E}}_{\left( {q,a}\right) \sim \mathcal{D},{\left\{ {o}_{i}\right\} }_{i = 1}^{G} \sim {\pi }_{{\theta }_{\text{old }}}\left( {\cdot \mid q}\right) } \left\lbrack {
{\color{teal}\frac{1}{\mathop{\sum }_{{i = 1}}^{G}\left| {o}_{i}\right| }\mathop{\sum }\limits_{{i = 1}}^{G}\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }}
\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},
{\color{brown}\operatorname{clip}\left( { {\color{black}{r}_{i,t}\left( \theta \right)} ,1 - {\varepsilon }_{\text{low }},1 + {\varepsilon }_{\text{high }}}\right)}
{\widehat{A}}_{i,t}}\right) }\right\rbrack, \\
\text{s.t. }\color{purple}0 <\left|\left\{{{o}_{i}\mid\texttt{is\_equivalent}({a,{o}_{i}})}\right\}\right|< G,\\
\text{where }{r}_{i,t}\left( \theta \right) = \frac{{\pi }_{\theta }\left( {{o}_{i,t} \mid q,{o}_{i, < t}}\right) }{{\pi }_{{\theta }_{\text{old }}}\left( {{o}_{i,t} \mid q,{o}_{i, < t}}\right) },\;{\widehat{A}}_{i,t} = \frac{{R}_{i} - \operatorname{mean}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }{\operatorname{std}\left( {\left\{ {R}_{i}\right\} }_{i = 1}^{G}\right) }.
\end{gathered}
$$

### 2.2. Clip-Higher

**Motivation**ï¼šä½œè€…å‘ç°ï¼Œåœ¨æœ´ç´ çš„ PPO æˆ– GRPO è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œç­–ç•¥çš„ç†µè¿…é€Ÿä¸‹é™(ç§°ä¸º **ç†µå´©æºƒ)** ï¼‰ï¼Œæ¨¡å‹ä¸§å¤±äº†æ¢ç´¢æ–°è¡Œä¸ºçš„èƒ½åŠ›ã€‚ä½œè€…å‘ç°ï¼Œä¸Šé™è£å‰ªåœ¨ç­–ç•¥æ¢ç´¢æ–¹é¢èµ·åˆ°äº†å‰¯ä½œç”¨ï¼š

> [!example] Example: ä¸Šé™è£å‰ªé™åˆ¶ä½æ¦‚ç‡ token çš„æ¦‚ç‡æå‡
>
> å‡è®¾è£å‰ªå‚æ•° $\varepsilon = 0.2$ï¼Œè€ƒè™‘ä¸¤ä¸ªåŠ¨ä½œï¼Œå®ƒä»¬çš„æ—§ç­–ç•¥æ¦‚ç‡ ${\pi }_{{\theta }_{\text{old }}}\left( {{o}_{i} \mid q}\right)$ åˆ†åˆ«ä¸º 0.01 å’Œ 0.9ã€‚
>
> -   å¯¹äºæ¦‚ç‡ä¸º 0.01 çš„åŠ¨ä½œï¼Œç»è¿‡ä¸Šé™è£å‰ªåï¼Œå…¶æ¦‚ç‡ ${\pi }_{\theta }\left( {{o}_{i} \mid q}\right)$ æœ€å¤§åªèƒ½æ›´æ–°åˆ° $0.01 \times (1 + 0.2) = 0.012$ã€‚
> -   å¯¹äºæ¦‚ç‡ä¸º 0.9 çš„åŠ¨ä½œï¼Œç»è¿‡ä¸Šé™è£å‰ªåï¼Œå…¶æ¦‚ç‡ ${\pi }_{\theta }\left( {{o}_{i} \mid q}\right)$ æœ€å¤§å¯ä»¥æ›´æ–°åˆ° $0.9 \times (1 + 0.2) = 1.08$ï¼Œä½†å®é™…æ¦‚ç‡å€¼ä¼šè¢«é™åˆ¶åœ¨ 1 ä»¥å†…ï¼Œå› æ­¤æœ€å¤§æ›´æ–°åˆ° 1ã€‚

**Solution**ï¼šä½œè€…æå‡ºå¯ä»¥ ==è§£è€¦è£å‰ªèŒƒå›´çš„ä¸Šä¸‹ç•Œ==ï¼Œå³ï¼šå°†è£å‰ªæ“ä½œ $\text{clip}(\cdot,1-\epsilon,1+\epsilon)$ æ›¿æ¢ä¸º $\color{brown}{\text{clip}(\cdot,1-\epsilon_{\text{low}},1+\epsilon_{\text{high}})}$ï¼Œä»è€Œç»™ä½æ¦‚ç‡ token ç•™å‡ºæ›´å¤§çš„æå‡ç©ºé—´ï¼Œæœ‰åŠ©äºæé«˜ç­–ç•¥ç†µã€‚

![|800](https://img.memset0.cn/2025/03/22/Bv9TmKCa.png)

### 2.3. Dynamic Sampling

**Motivation**ï¼šåœ¨ç°æœ‰çš„ PPO æˆ– GRPO çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚æœæ¨¡å‹åœ¨æŸäº›ç‰¹å®š prompt ä¸Šè¡¨ç°ç‰¹åˆ«å¥½ï¼Œç”Ÿæˆçš„æ‰€æœ‰å›å¤éƒ½æ˜¯æ­£ç¡®æ—¶ï¼Œå°±ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œå½±å“è®­ç»ƒã€‚

**Solution**ï¼šä½œè€…æå‡º **åŠ¨æ€é‡‡æ ·(dynamic sampling)** ç­–ç•¥ï¼Œæ ¸å¿ƒåœ¨äº **è¿‡é‡‡æ ·(oversampling)** å’Œ **è¿‡æ»¤(filtering)**ã€‚é€šè¿‡é¢å¤–é‡‡æ ·å¹¶ç”¨çº¦æŸæ¡ä»¶ $\color{purple}0 <\left|\left\{{{o}_{i}\mid\texttt{is\_equivalent}({a,{o}_{i}})}\right\}\right|< G$ è¿‡æ»¤æ‰å›å¤å…¨éƒ¨æ­£ç¡®æˆ–å…¨éƒ¨é”™è¯¯çš„æ ·æœ¬ï¼Œä»è€Œè®©æ¯ä¸ª batch éƒ½å……æ»¡æ¢¯åº¦ä¸ä¸º $0$ çš„æ ·æœ¬ã€‚

ä½œè€…è¿˜æŒ‡å‡ºï¼ŒDynamic Sampling ç­–ç•¥ä¸ä¸€å®šä¼šé™ä½è®­ç»ƒæ•ˆç‡ã€‚å› ä¸ºåœ¨ä¸é‡‡ç”¨æµæ°´çº¿æŠ€æœ¯æˆ–å¹¶è¡ŒæŠ€æœ¯çš„é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€ batch çš„ç”Ÿæˆæ—¶é—´é€šå¸¸æ˜¯ç”±ç”Ÿæˆæ—¶é—´å¼‚å¸¸é•¿çš„â€œ**é•¿å°¾æ ·æœ¬(long-tail sample)**â€æ‰€ä¸»å¯¼çš„ï¼Œè€Œâ€œè¿‡é‡‡æ ·â€æ­¥éª¤å¤§æ¦‚ç‡åªä¼šå¢åŠ è½»é‡æ ·æœ¬çš„é‡‡æ ·ï¼Œè€Œä¸ä¼šæ˜æ˜¾å¢åŠ é‡‡æ ·æ—¶é—´ã€‚

![|800](https://img.memset0.cn/2025/03/22/eu9Lf2xV.png)

### 2.4. Token-Level Policy Gradient Loss

**Motivation**ï¼šåœ¨ GPRO ä¸­æˆ‘ä»¬é‡‡ç”¨äº† sample-level çš„æŸå¤±å¹³å‡ $\displaystyle{\dfrac{1}{G}\sum_{i=1}^{G} \dfrac{1}{|o_{i}|}\sum_{i=1}^{|o_{i}|}}$ï¼Œè¿™å°†å¯¼è‡´å¯¹äºé•¿æ ·æœ¬ï¼Œæ¯ä¸ª token å¯¹æ€»ä½“çš„æŸå¤±ä¼šè¢«ç¨€é‡Šã€‚è€Œä½œè€…å‘ç°ï¼Œ==è¿‡é•¿çš„æ ·æœ¬å°å°ä¼šå‡ºç°ä½è´¨é‡æ¨¡å¼==ï¼Œå¦‚èƒ¡è¨€ä¹±è¯­å’Œé‡å¤è¯æ±‡ï¼Œä½œè€…å¸Œæœ›èƒ½æœ‰æ•ˆæƒ©ç½šè¿™äº›ä¸è‰¯æ¨¡å¼ã€‚

**Solution**ï¼šåœ¨æœ¬è®ºæ–‡ä¸­ä½œè€…é‡‡ç”¨ token-level çš„æŸå¤±å¹³å‡ $\displaystyle{{\color{teal}\frac{1}{\mathop{\sum }_{{i = 1}}^{G}\left| {o}_{i}\right| }\mathop{\sum }\limits_{{i = 1}}^{G}\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }}}$ï¼Œè¿™æ ·é•¿æ ·æœ¬å¯¹æ€»ä½“æ¢¯åº¦æ›´æ–°çš„å½±å“æ›´å¤§ã€‚å¦å¤–ä½œè€…æŒ‡å‡ºï¼Œå¯¹äºå•ä¸ªè¯å…ƒï¼Œå¦‚æœæŸç§ç‰¹å®šçš„ç”Ÿæˆæ¨¡å¼èƒ½å¯¼è‡´å¥–åŠ±å¢åŠ æˆ–å‡å°‘ï¼Œæ— è®ºå®ƒå‡ºç°åœ¨å¤šé•¿çš„å“åº”ä¸­ï¼Œéƒ½ä¼šå—åˆ°åŒç­‰ç¨‹åº¦çš„æ¿€åŠ±æˆ–æŠ‘åˆ¶ã€‚

### 2.5. Soft Overlong Punishment

**Motivation**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯èƒ½è®¾æˆè¿‡é•¿çš„æ–‡æœ¬è€Œéœ€è¦è¿›è¡Œæˆªæ–­å¤„ç†ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¸¸è§çš„åšæ³•æ˜¯ç»™è¢«æˆªæ–­çš„æ ·æœ¬åˆ†é…ä¸€ä¸ª**æƒ©ç½šæ€§å¥–åŠ±(punitive reward)**ã€‚ç„¶è€Œï¼Œä½œè€…å‘ç°è¿™ç§æƒ©ç½šä¼šå¸¦æ¥ **å¥–åŠ±å™ªå£°(reward noise)**ï¼Œå³ä¸€ä¸ªä¼˜ç§€çš„æ¨ç†è¿‡ç¨‹å¯èƒ½å› ä¸ºå…¶ç”Ÿæˆäº†è¿‡é•¿çš„æ–‡æœ¬è€Œè¢«æƒ©ç½šã€‚

**Solution**ï¼šä½œè€…é‡‡å–äº†ä¸€ç§ **è½¯æ€§è¿‡é•¿æƒ©ç½š(soft overlong punishment)** çš„ç­–ç•¥ï¼Œè®¾ç½®æœ€å¤§ç”Ÿæˆé•¿åº¦ $L_{\text{max}}$ ä¸â€œè½¯æƒ©ç½šç¼“å†²é•¿åº¦â€ $L_{\text{cache}}$ ä»è€Œå¹³è¡¡æƒ©ç½šæ€§æŸå¤±å’Œç›´æ¥å±è”½æŸå¤±çš„æ–¹æ³•ï¼š

$$
{R}_{\text{length }}\left( y\right) = \left\{ \begin{array}{ll} 0, & \left| y\right| \leq {L}_{\max } - {L}_{\text{cache }} \\ \frac{\left( {{L}_{\max } - {L}_{\text{cache }}}\right) - \left| y\right| }{{L}_{\text{cache }}}, & {L}_{\max } - {L}_{\text{cache }} < \left| y\right| \leq {L}_{\max } \\ - 1, & {L}_{\max } < \left| y\right| \end{array}\right.
$$

## 3. Experiments

### 3.1. Dataset Transformation

å¯¹äºæ•°å­¦é¢˜ç›®æ•°æ®é›†ï¼Œç”±äºç­”æ¡ˆæ ¼å¼å¤šç§å¤šæ ·ï¼Œä¸ºäº†æ–¹ä¾¿å®ç°ç­‰ä»·æ€§åˆ¤æ–­å‡½æ•° $\texttt{is\_equivalent}(\hat{y},y)$ çš„å®ç°ï¼Œä½œè€…å—åˆ°ç¾å›½æ•°å­¦ç«èµ› AIME çš„å¯å‘ï¼Œå°†ç­”æ¡ˆå…¨éƒ¨è½¬åŒ–ä¸ºæ•´æ•°æ¥è§£æå¹¶æ¯”è¾ƒï¼Œä»è€Œå¾—åˆ°äº† DAPO-Math-17K æ•°æ®é›†ã€‚

> [!example]- å°†ç­”æ¡ˆè½¬åŒ–ä¸ºæ•´æ•°çš„ä¾‹å­
>
> **åŸå§‹é—®é¢˜(Original Problem)**ï¼š
>
> Let $x$ and $y$ be real numbers such that ${x}^{2} + {y}^{2} - {22x} - {16y} + {113} = 0$ . Determine the smallest possible value of $x$ .
>
> Answer: ${11} - 2\sqrt{6}$
>
> **è½¬æ¢åçš„é—®é¢˜(Transformed Problem)**ï¼š
>
> Let $x$ and $y$ be real numbers such that ${x}^{2} + {y}^{2} - {22x} - {16y} + {113} = 0$ . Determine the smallest possible value of $x$ . The original answer is in the form $k - m\sqrt{n}$ ,where $k,m$ ,and $n$ are integers. Please find the value of $k + m + n$ .
>
> Answer: 19






