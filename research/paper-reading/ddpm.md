---
title: "ã€Œè®ºæ–‡ç²¾è¯» #7ã€Denoising Diffusion Probabilistic Models"
create-date: 2025-02-13 20:06:52
update-date: 2025-02-16 01:57:00
slug: /research/paper-reading/ddpm
indexed: true
tags:
  - Diffusion-Model
  - Gaussian-distribution
  - Markov-chain
  - KL-Divergence
citekey: hoDenoisingDiffusionProbabilistic2020
doi: 10.48550/arXiv.2006.11239
export-date: 2025-02-23 01:58:40
---

<!-- begin-private-notes -->

> [!summary] Metadata
>
> **Title**: [Denoising Diffusion Probabilistic Models](zotero://open-pdf/library/items/V37FIZ93)
>
> **Tags**: #zotero/tag/Computer-Science---Machine-Learning, #zotero/tag/Statistics---Machine-Learning
>
> **Authors**: #zotero/author/Jonathan-Ho, #zotero/author/Ajay-Jain, #zotero/author/Pieter-Abbeel

%% begin notes %%

<!-- end-private-notes -->

> æœ¬ç¯‡ç¬”è®°æ·±å…¥è§£æäº† Denoising Diffusion Probabilistic Models (DDPM) çš„åŸç†ã€‚é¦–å…ˆä»‹ç»äº†æ•°å­¦ç¬¦å·å’ŒåŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼ˆå‰å‘é©¬å°”å¯å¤«è¿‡ç¨‹ï¼‰ä»¥åŠé€†å‘å»å™ªè¿‡ç¨‹ï¼ˆåå‘é©¬å°”å¯å¤«è¿‡ç¨‹ï¼‰ã€‚ç„¶åè¯¦ç»†æ¨å¯¼äº†å˜åˆ†ä¸‹ç•Œï¼ˆVLBï¼‰çš„æ¨å¯¼è¿‡ç¨‹ï¼Œå¹¶åˆ†æäº†æ¨¡å‹çš„æŸå¤±å‡½æ•°æ„é€ ï¼Œç‰¹åˆ«æ˜¯ KL æ•£åº¦ä¼˜åŒ–ç›®æ ‡ã€‚ç¬”è®°è¿˜æ¶µç›–äº†è®­ç»ƒè¿‡ç¨‹çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶ç»™å‡ºäº†åå‘é‡‡æ ·ç®—æ³•çš„å®ç°æ–¹æ³•ï¼Œå¸®åŠ©è¯»è€…ç†è§£æ‰©æ•£æ¨¡å‹çš„å®é™…è¿ä½œæ–¹å¼ã€‚æœ€åï¼Œé™„ä¸Šç›¸å…³é“¾æ¥ä¾›è¿›ä¸€æ­¥é˜…è¯»ã€‚<small style="font-style: italic; opacity: 0.5">ï¼ˆç”± gpt-4o ç”Ÿæˆæ‘˜è¦ï¼‰</small>

<!-- more -->

| Notation                                                    | Description                                                              | Comment                                                                                    |
| ----------------------------------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| $\mathbf{x_{0}}$                                            | çœŸå®æ•°æ®                                                                 |                                                                                            |
| $q(\mathbf{x}_{0})$                                         | çœŸå®æ•°æ®åˆ†å¸ƒ                                                             | $\mathbf{x_{0}} \sim q(\mathbf{x_{0}})$ è¡¨ç¤º $\mathbf{x_{0}}$ æ˜¯ä»çœŸå®æ•°æ®åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„ |
| $q(x\vert y)$                                               | çœŸå®æ•°æ®ä¸­ç»™å®š $y$ æ—¶ $x$ çš„åˆ†å¸ƒ                                         |                                                                                            |
| $\mathbf{x}_{1} \cdots \mathbf{x}_{T}$                      | **éšå˜é‡(latent variable)**                                              | ä¸ $\mathbf{x}_{0}$ å…·æœ‰ç›¸åŒçš„ç»´åº¦                                                         |
| $\theta$                                                    | å¯å­¦ä¹ å‚æ•°                                                               |                                                                                            |
| $p_\theta(\mathbf{x}_{0})$                                  | æ¨¡å‹æ•°æ®åˆ†å¸ƒ                                                             | æˆ‘ä»¬å¸Œæœ›é€šè¿‡ $p_{\theta}(\mathbf{x}_0)$ å»é€¼è¿‘ $q(\mathbf{x}_{0})$                         |
| $\mathcal{N}(\cdot; \boldsymbol{\mu}, \boldsymbol{\Sigma})$ | å‡å€¼ä¸º $\boldsymbol{\mu}$ï¼Œåæ–¹å·®çŸ©é˜µä¸º $\boldsymbol{\Sigma}$ çš„é«˜æ–¯åˆ†å¸ƒ |                                                                                            |

## 1. Insights

### 1.1. Background

æ‰©æ•£æ¨¡å‹æ˜¯å½¢å¦‚

$$
p_{\theta}(\mathbf{x}_{0}) := \int p_{\theta}(\mathbf{x}_{0:T}) \text{d}  \mathbf{x}_{1:T}
$$

> è¯¥å…¬å¼è¡¨æ˜ï¼Œæ•°æ® $\mathbf{x}_0$ çš„è¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒ $p_\theta(\mathbf{x}_0)$ å¯ä»¥é€šè¿‡å¯¹è”åˆæ¦‚ç‡åˆ†å¸ƒ $p_\theta(\mathbf{x}_{0:T})$ ä¸­æ‰€æœ‰çš„éšå˜é‡ $\mathbf{x}_{1:T} = (\mathbf{x}_1, \ldots, \mathbf{x}_T)$ è¿›è¡Œç§¯åˆ†ï¼ˆè¾¹ç¼˜åŒ–ï¼‰å¾—åˆ°ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå…³äº $\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_T$ çš„è”åˆåˆ†å¸ƒï¼Œç„¶åé€šè¿‡ç§¯åˆ†æ‰æˆ‘ä»¬ä¸å…³å¿ƒçš„å˜é‡ $\mathbf{x}_{1},\cdots,\mathbf{x}_{T}$ å°±å¾—åˆ°äº†æˆ‘ä»¬æœ€ç»ˆæƒ³è¦å»ºæ¨¡çš„æ•°æ®åˆ†å¸ƒ $p_\theta(\mathbf{x}_0)$ã€‚

çš„ **éšå˜é‡æ¨¡å‹(latent variable model)**ã€‚å…¶ä¸­ $p_{\theta}(\mathbf{x}_{0:T})$ è¡¨ç¤º **åå‘è¿‡ç¨‹(reverse process)**ã€‚åå‘è¿‡ç¨‹æ¨¡æ‹Ÿçš„æ˜¯ä»å™ªå£°é€æ¸æ¢å¤åˆ°æ•°æ®çš„è¿‡ç¨‹ï¼Œæ–¹å‘ä¸â€œæ‰©æ•£â€è¿‡ç¨‹ç›¸åã€‚åå‘è¿‡ç¨‹è¢«å®šä¹‰ä¸ºä¸€ä¸ªå…·æœ‰ **å¯å­¦ä¹ é«˜æ–¯è½¬ç§»(learned Gaussian transition)** çš„ **é©¬å°”å¯å¤«é“¾(Markov chain)**ï¼Œè¿™æ„å‘³ç€åœ¨ç»™å®šå½“å‰çŠ¶æ€ $\mathbf{x}_t$ çš„æ¡ä»¶ä¸‹ï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€ $\mathbf{x}_{t-1}$ çš„æ¦‚ç‡åˆ†å¸ƒåªä¾èµ–äº $\mathbf{x}_t$ï¼Œè€Œä¸æ›´æ—©çš„çŠ¶æ€æ— å…³ï¼Œå¹¶ä¸”è¿™ç§è½¬ç§»æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ï¼Œå…¶å‚æ•°æ˜¯å¯å­¦ä¹ çš„ã€‚æ ¹æ®è¿™ä¸€å®šä¹‰å¯ä»¥æŠŠè”åˆæ¦‚ç‡åˆ†å¸ƒæ‹†è§£ä¸ºï¼š

$$
p_\theta(\mathbf{x}_{0:T}) := p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t), \quad \text{where}\space p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) := \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
$$

æ‰©æ•£æ¨¡å‹çš„ç‰¹ç‚¹åœ¨äºï¼Œå…¶ **å‰å‘è¿‡ç¨‹(forward process)** æˆ–è€…ç§°ä¸º **æ‰©æ•£è¿‡ç¨‹(diffusion process)** $q(\mathbf{x}_{1:T} | \mathbf{x}_{0})$ æ˜¯å›ºå®šçš„é©¬å°”å¯å¤«è¿‡ç¨‹ï¼Œä¸éœ€è¦å‚æ•°å­¦ä¹ ï¼Œä¸”æ¯ä¸€æ­¥éƒ½å¯ä»¥çœ‹æˆæ˜¯å‘æ•°æ®ä¸­æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œé€šè¿‡æ–¹å·®è¡¨ $\beta_1, \ldots, \beta_T$ æ§åˆ¶ï¼Œé€šå¸¸ $\beta_1, \ldots, \beta_T$ æ˜¯ä¸€ä¸ªé€’å¢çš„åºåˆ—ï¼Œæ„å‘³ç€éšç€æ—¶é—´æ­¥ $t$ çš„å¢å¤§ï¼Œæ·»åŠ çš„å™ªå£°ä¹Ÿè¶Šæ¥è¶Šå¤šï¼›è¿™ä¸€å‚æ•°å¯ä»¥å­¦ä¹ æˆ–å›ºå®šï¼Œä¸ºäº†æ–¹ä¾¿é€šå¸¸é€‰ç”¨è¶…å‚æ•°ã€‚

$$
q(\mathbf{x}_{1:T}|\mathbf{x}_0) := \prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1}), \quad \text{where} \space q(\mathbf{x}_t|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
$$

> -   $\sqrt{1 - \beta_t}$ å› å­ç”¨äºåœ¨æ¯ä¸€æ­¥ä¸­å¯¹ä¿¡å·è¿›è¡Œç¼©æ”¾ï¼Œä»¥ä¿æŒæ•°æ®å’Œå™ªå£°çš„ç›¸å¯¹æ¯”ä¾‹ã€‚
> -   å³å¼å¯ä»¥æ ¹æ®é«˜æ–¯åˆ†å¸ƒçš„å®šä¹‰å±•å¼€ï¼š$\mathbf{x}_{t} = \sqrt{1-\beta_{t}} \mathbf{x}_{t-1} +\sqrt{\beta_{t}} \boldsymbol{\epsilon}_{t}$ï¼Œå…¶ä¸­ $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ã€‚

ä¸ºæ–¹ä¾¿ï¼Œè®° $\alpha_{t} = 1-\beta_{t}$ï¼Œ$\bar{\alpha}_{t}\mathrel{\text{:=}} \prod_{s = 1}^{t}{\alpha }_{s}$ï¼Œå¯ä»¥æ¨å¯¼å‡º $q(\mathbf{x}_{t}|\mathbf{x}_{0})$ çš„==å°é—­å½¢å¼==ï¼›è¿™è¡¨æ˜æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸€å°é—­å½¢å¼åœ¨ä»»æ„æ—¶é—´æ­¥ $t$ å¯¹ $\mathbf{x}_{t}$ è¿›è¡Œé‡‡æ ·ï¼›é€šä¿—ç‚¹è¯´ï¼Œè¿™ä¸€æ¨å¯¼æŒ‡å‡ºé©¬å°”ç§‘å¤«æ­£å‘è¿‡ç¨‹çš„ä»»æ„æ—¶åˆ»éƒ½å¯ä»¥ä¸€æ­¥åˆ°ä½ã€‚åé¢ä¹Ÿä¼šå¤šæ¬¡ç”¨åˆ°è¿™ä¸ªå°é—­å½¢å¼ã€‚

$$
q(\mathbf{x}_{t} | \mathbf{x}_{0}) \sim \mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha_{t}}} \mathbf{x}_{0}, (1-\bar{\alpha}_{t}) \mathbf{I} )
$$

> [!quote]- Proof
>
> è¿™å…¶å®å°±æ˜¯åˆ©ç”¨é©¬å°”ç§‘å¤«é“¾çš„æ€§è´¨è¿­ä»£å±•å¼€ï¼Œæ ¹æ®å‰é¢çš„å®šä¹‰å¼æœ‰ï¼š
>
> $$
> \begin{gathered}
> \mathbf{x}_{t} = \sqrt{\alpha_{t}} \mathbf{x}_{t-1} + \sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t-1}\\
> \mathbf{x}_{t-1} = \sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}
> \end{gathered}
> $$
>
> ä»£å…¥å¾—ï¼š
>
> $$
> \begin{aligned}
> \mathbf{x}_{t} &= \sqrt{\alpha_{t}} (\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}  ) + \sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t-1} \\
> &= \sqrt{\alpha_{t} \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t-1} + \sqrt{\alpha_{t}(1-\alpha_{t-1})} \boldsymbol{\epsilon}_{t-2}
> \end{aligned}
> $$
>
> å…¶ä¸­ï¼Œå› ä¸ºæ ¹æ®é«˜æ–¯åˆ†å¸ƒçš„æ€§è´¨ $\mathcal{N}(\mathbf{0},\sigma_{1}^{2}\mathbf{I}) + \mathcal{N}(\mathbf{0},\sigma_{2}^{2}\mathbf{I}) \sim \mathcal{N}(\mathbf{0},(\sigma_{1}^{2},\sigma_{2}^{2})\mathbf{I})$ å¯ä»¥å¾—åˆ°ï¼š
>
> $$
> \mathbf{x}_{t} = \sqrt{\alpha_{t} \alpha}_{t-1} \mathbf{x}_{t-2}  + \sqrt{1-\alpha_{t} \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2}
> $$
>
> å‘å‰è¿­ä»£ç›´åˆ° $\mathbf{x}_{0}$ å³å¯è¯æ˜ç›®æ ‡ç»“è®ºï¼š
>
> $$
> \mathbf{x}_{t} = \sqrt{\bar{\alpha}_{t}}  \mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{t}
> $$

### 1.2. DDPM

æ‰©æ•£æ¨¡å‹æ˜¯==é€šè¿‡å¯»æ‰¾ä½¿è®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§æœ€å¤§åŒ–åå‘é©¬å°”å¯å¤«è½¬ç§»æ¥è®­ç»ƒçš„==ã€‚åœ¨å®è·µä¸­ï¼Œè¿™ç­‰åŒäºå°†ç›®æ ‡å‡½æ•°è®¾å®šä¸º==æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶çš„å˜åˆ†ä¸Šç•Œ==ã€‚

$$
\begin{aligned}
p_\theta(\mathbf{x}_0) &=\int p_\theta(\mathbf{x}_{0:T}) d\mathbf{x}_{1:T} =\int \frac{p_\theta(\mathbf{x}_{0:T}) q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})}{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} d\mathbf{x}_{1:T}\\
&\geq \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \left[ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \right] =: \mathcal{L}_{\mathrm{VLB}} \\ \end{aligned}
$$

> -   æœ€åä¸€æ­¥åˆ©ç”¨äº† [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen's_inequality)ã€‚

$$
\mathcal{L} := \mathbb{E}(-\log p_{\theta}(\mathbf{x}_{0})) = - \log \mathcal{L}_{\mathrm{VLB}} = \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})}[-\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})}]=\mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})}[\log \frac{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0:T})}]
$$

é€šè¿‡æ¨å¯¼å¯ä»¥å¾—åˆ°åŸºäº KL æ•£åº¦çš„æŸå¤±å‡½æ•°ï¼š

$$
\mathcal{L}:={\mathbb{E}}_{q}\left\lbrack {\underset{{L}_{T}}{\underbrace{{D}_{\mathrm{{KL}}}\left( {q\left( {{\mathbf{x}}_{T} | {\mathbf{x}}_{0}}\right) \parallel p\left( {\mathbf{x}}_{T}\right) }\right) }} + \mathop{\sum }\limits_{{t > 1}}\underset{{L}_{t - 1}}{\underbrace{{D}_{\mathrm{{KL}}}\left( {q\left( {{\mathbf{x}}_{t - 1} | {\mathbf{x}}_{t},{\mathbf{x}}_{0}}\right) \parallel {p}_{\theta }\left( {{\mathbf{x}}_{t - 1}|{\mathbf{x}}_{t}}\right) }\right) }}\underset{{L}_{0}}{\underbrace{-\log {p}_{\theta }\left( {{\mathbf{x}}_{0}|{\mathbf{x}}_{1}}\right) }}}\right\rbrack
$$

> -   è®­ç»ƒæ—¶ $L_{T}$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¯ä»¥å¿½ç•¥ã€‚

> [!quote]- Proof
>
> ï¼ˆè½¬è½½è‡ªï¼š[æ‰©æ•£æ¨¡å‹ä¹‹ DDPM - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/563661713)ï¼‰
>
> $$
> \begin{aligned} \mathcal{L} &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1}, \mathbf{x}_{0})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] & \text{ ;use } q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0)=q(\mathbf{x}_t \vert \mathbf{x}_{t-1})\\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] & \text{ ;use Bayes' Rule }\\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big]\\ &= \mathbb{E}_{q(\mathbf{x}_{1:T}\vert \mathbf{x}_{0})} \Big[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{T}\vert \mathbf{x}_{0})}\Big[\log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)}\Big]+\sum_{t=2}^T \mathbb{E}_{q(\mathbf{x}_{t}, \mathbf{x}_{t-1}\vert \mathbf{x}_{0})}\Big[\log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\Big] - \mathbb{E}_{q(\mathbf{x}_{1}\vert \mathbf{x}_{0})}\Big[\log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)\Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{T}\vert \mathbf{x}_{0})}\Big[\log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)}\Big]+\sum_{t=2}^T \mathbb{E}_{q(\mathbf{x}_{t}\vert \mathbf{x}_{0})}\Big[q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)\log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\Big] - \mathbb{E}_{q(\mathbf{x}_{1}\vert \mathbf{x}_{0})}\Big[\log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)\Big] \\ &= \underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{\mathbb{E}_{q(\mathbf{x}_{t}\vert \mathbf{x}_{0})}\Big[D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))\Big]}_{L_{t-1}} -\underbrace{\mathbb{E}_{q(\mathbf{x}_{1}\vert \mathbf{x}_{0})}\log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} \end{aligned}
> $$

ç”±æ­¤å·²ç»å¾—åˆ°äº†æ­£å‘è¿‡ç¨‹çš„å¼å­ï¼Œä½†æˆ‘ä»¬çš„ç›®çš„æ˜¯å»å­¦ä¹ åå‘è¿‡ç¨‹ï¼ˆå»å™ªï¼‰ã€‚ç”±äºåå‘è¿‡ç¨‹ä¹Ÿæ˜¯é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼Œå¯ä»¥æ‹†è§£ä¸ºæ¦‚ç‡çš„è¿ä¹˜ç§¯ï¼Œå…¶ä¸­çš„æ¯ä¸€é¡¹å¯ä»¥é€šè¿‡è´å¶æ–¯å…¬å¼æ¨å¯¼å¾—åˆ°ï¼š

$$
\begin{aligned}
&q(\mathbf{x}_{t-1} | \mathbf{x}_{t} , \mathbf{x}_{0}) \sim \mathcal{N}( \mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_{t},\mathbf{x}_{0}), \tilde{\beta}_{t} \mathbf{I}),\\
\text{where}\quad&\tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_{t}, \mathbf{x}_{0}) := \dfrac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t} }{1-\bar{\alpha}_{t}} \mathbf{x}_{0} + \dfrac{\sqrt{\alpha_{t}} (1-\bar{\alpha}_{t-1}) }{1 - \bar{\alpha}_{t}} \mathbf{x}_{t}\\
\text{and}\quad &\tilde{\beta}_{t} := \dfrac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \beta_{t}
\end{aligned}
$$

> [!quote]- Proof
>
> é¦–å…ˆæ ¹æ®==è´å¶æ–¯å…¬å¼==ï¼š
>
> $$
> q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0}) = \dfrac{q(\mathbf{x}_{t} | \mathbf{x}_{t-1} ,\mathbf{x}_{0}) q(\mathbf{x}_{t-1} | \mathbf{x}_{0})}{q(\mathbf{x}_{t} | \mathbf{x_{0}})}
> $$
>
> æ ¹æ®å‰é¢çš„æ¨å¯¼å¯ä»¥å¾—åˆ°ï¼š
>
> $$
> \begin{aligned}
> q(\mathbf{x}_{t-1}| \mathbf{x}_{0}) &\sim \mathcal{N}(\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}, (1-\bar{\alpha}_{t-1}) \mathbf{I})\\
> q(\mathbf{x}_{t} | \mathbf{x}_{0}) &\sim \mathcal{N}(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}, (1 - \bar{\alpha}_{t}) \mathbf{I} )\\
> q(\mathbf{x}_{t} | \mathbf{x}_{t-1}, \mathbf{x}_{0}) &\sim \mathcal{N}(\sqrt{\alpha_{t}} \mathbf{x}_{t-1}, (1 - \alpha_{t}) \mathbf{I})
> \end{aligned}
> $$
>
> ä¸€ä¸ªæŠ€å·§æ˜¯æˆ‘ä»¬åªè€ƒè™‘é«˜æ–¯åˆ†å¸ƒçš„æŒ‡æ•°éƒ¨åˆ†ï¼Œå¿½ç•¥å½’ä¸€åŒ–å¸¸æ•°ï¼š
>
> $$
> \mathcal{N}( {\mu ,{\sigma }^{2}})  \propto  \exp \left( {-\frac{{\left( x - \mu \right) }^{2}}{2{\sigma }^{2}}}\right)  = \exp \left( { - \frac{1}{2}\left( {\frac{1}{{\sigma }^{2}}{x}^{2} - \frac{2\mu }{{\sigma }^{2}}x + \frac{{\mu }^{2}}{{\sigma }^{2}}}\right) }\right)
> $$
>
> ä¸€æ–¹é¢å¯ä»¥åˆ©ç”¨è¿™ä¸€å…¬å¼å±•å¼€è´å¶æ–¯å…¬å¼ä¸­çš„æ¦‚ç‡é¡¹ï¼›==å¦ä¸€æ–¹é¢ï¼Œå¦‚æœé€šè¿‡è´å¶æ–¯åˆ†å¸ƒå¾—åˆ°çš„åˆ†å¸ƒçš„æœ‰å›ºå®šå½’ä¸€åŒ–å¸¸æ•°ä¸”æŒ‡æ•°åˆ†å¸ƒä¹Ÿå¯ä»¥é…æ–¹æˆè¿™æ ·çš„å½¢å¼ï¼Œé‚£ä¹ˆå…¶åº”è¯¥ä»æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒ==ã€‚ä¸‹é¢è¿›è¡Œä¸€ä¸ªæ¨å¯¼ï¼ˆä¸ºäº†æ–¹ä¾¿ä»…è®¡ç®—å‘é‡ $\mathbf{x}_{t}$ ä¸­å¯¹åº”ä½ç½®çš„ä¸€é¡¹ $x_{t}$ï¼‰ï¼š
>
> $$
> \begin{aligned}
> q(x_{t-1}|x_{t},x_{0})
> &\propto  \exp \left( {-\frac{1}{2}\left( {\frac{{\left( {x}_{t} - \sqrt{{\alpha }_{t}}{x}_{t - 1}\right) }^{2}}{{\beta }_{t}} + \frac{{\left( {x}_{t - 1} - \sqrt{{\bar{\alpha }}_{t - 1}}{x}_{0}\right) }^{2}}{1 - {\bar{\alpha }}_{t - 1}} - \frac{{\left( {x}_{t} - \sqrt{{\bar{\alpha }}_{t}}{x}_{0}\right) }^{2}}{1 - {\bar{\alpha }}_{t}}}\right) }\right)\\
> &=\exp \left( {-\frac{1}{2}\left( {\left( {\frac{{\alpha }_{t}}{{\beta }_{t}} + \frac{1}{1 - {\bar{\alpha }}_{t - 1}}}\right) {x}_{t - 1}^{2} - \left( {\frac{2\sqrt{{\alpha }_{t}}}{{\beta }_{t}}{x}_{t} + \frac{2\sqrt{{\bar{\alpha }}_{t - 1}}}{1 - {\bar{\alpha }}_{t - 1}}{x}_{0}}\right) {x}_{t - 1} + C\left( {{x}_{t},{x}_{0}}\right) }\right) }\right)
> \end{aligned}
> $$
>
> è¿™é‡Œ $C(x_{t},x_{0})$ è¡¨ç¤ºæ˜¯å…³äº $x_{t},x_{0}$ çš„å¸¸æ•°é¡¹ï¼Œè¿™é‡Œä¸ºäº†æ–¹ä¾¿å…ˆçœç•¥ã€‚æ ¹æ® $x_{t-1}^{2}$ å’Œ $x_{t-1}$ é¡¹çš„ç³»æ•°å¯ä»¥æ¨å‡ºå‡å€¼ $\tilde{\mu}_{t}$ å’Œæ–¹å·® $\tilde{\beta}_{t}$ï¼Œå¹¶ä¸”å¯ä»¥éªŒè¯æœ€åçš„å¸¸æ•°é¡¹ä¹Ÿç¬¦åˆä¸Šé¢é«˜æ–¯åˆ†å¸ƒçš„å¼å­ã€‚æœ€åæ‰€å¾—ç»“æœä¸ºï¼š
>
> $$
> \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_t,\quad \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t
> $$

è¿™è¯´æ˜ï¼ŒçœŸå®æ•°æ®çš„åå‘è¿‡ç¨‹ $q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})$ ä»ç„¶æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸”å‡å€¼å’Œæ–¹å·®å…·æœ‰å°é—­å½¢å¼ï¼Œå…¶ä¸­å‡å€¼ $\tilde{\boldsymbol{\mu}}_t$ æ˜¯å…³äº $\boldsymbol{x}_{t},\boldsymbol{x}_{0}$ çš„å‡½æ•°ï¼Œæ–¹å·® $\tilde{\beta}_{t}$ æ˜¯å¸¸æ•°ã€‚

ä¸è¿‡æ³¨æ„åˆ°ï¼Œ==$q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})$ å®é™…ä¸Šä¾èµ–äºéšæœºæ ·æœ¬ $\mathbf{x}_{0}$ï¼Œè€Œè·å–çœŸå®çš„é€†è¿‡ç¨‹ $q(\mathbf{x}_{t-1}|\mathbf{x}_{t})$ æ˜¯å›°éš¾çš„==ï¼Œå› æ­¤é€šè¿‡ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹å…¶è¿›è¡Œè¿‘ä¼¼ï¼š

$$
p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t}) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_{\theta}(\mathbf{x}_{t},t), \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_{t},t))
$$

ç°åœ¨æˆ‘ä»¬è®¨è®ºæ¨¡å‹ $p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})$ çš„å…·ä½“ **å‚æ•°åŒ–æ–¹å¼**ï¼šå¯¹äº $\boldsymbol{\Sigma}_{\theta}$ï¼Œæœ¬è®ºæ–‡ä¸­ä½œè€…è®¤ä¸ºè®¾è®¡ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„å›ºå®šå€¼å°±èƒ½å¾—åˆ°è¾ƒå¥½çš„æ•ˆæœï¼Œåç»­å·¥ä½œ iDDPM ä¸­æŠŠè¿™ä¸€é¡¹ä¹Ÿä¼˜åŒ–æˆå¯è®­ç»ƒçš„äº†ï¼š

| $\boldsymbol{\Sigma}_{\theta}(\mathbf{x}_{t},t)$ çš„å–å€¼                         | ç†ç”±                                     | æ•ˆæœ                                                                   |
| ------------------------------------------------------------------------------- | ---------------------------------------- | ---------------------------------------------------------------------- |
| $\sigma_{t}^{2} = \beta_{t}$                                                    | ç›´æ¥å°†åå‘è¿‡ç¨‹çš„æ–¹å·®è®¾ç½®ä¸ºå‰å‘è¿‡ç¨‹çš„æ–¹å·® | å¯¹äº $\mathbf{x}_{0} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ æ˜¯æœ€ä¼˜çš„ |
| $\sigma^{2}_{t} = \dfrac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} = \beta_{t}$ | ä½¿ç”¨åŸºäºè´å¶æ–¯å…¬å¼æ¨å¯¼å‡ºçš„ä¿®æ­£åçš„æ–¹å·®   | å¯¹äº $\mathbf{x}_{0}$ ç¡®å®šæ€§è®¾ç½®ä¸ºä¸€ä¸ªç‚¹æ˜¯æœ€ä¼˜çš„                       |

åŸºäºè¿™ä¸€å›ºå®šæ–¹å·®çš„å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ¨å¯¼æŸå¤±å‡½æ•°ä¸­çš„ $L_{t-1}$ é¡¹ï¼ˆæ ¹æ®å‰æ–‡ï¼Œè¿™æ˜¯é€šè¿‡ KL æ•£åº¦å®šä¹‰çš„ï¼‰ï¼Œç”±äºè¿™é‡Œçš„ $q\left( {{\mathbf{x}}_{t - 1} | {\mathbf{x}}_{t},{\mathbf{x}}_{0}}\right)$ å’Œ ${p}_{\theta }\left( {{\mathbf{x}}_{t - 1}|{\mathbf{x}}_{t}}\right)$ éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒä¸”æ–¹å·®ä¸ºå¸¸æ•°é¡¹ï¼Œå¯ç®€åŒ–å¾—åˆ°ï¼š

$$
\mathcal{L}_{t-1} = D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) = \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \frac{1}{2\sigma_t^2} || \boldsymbol{\mu}_{t}(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) ||^2 \right] + C
$$

> -   è¿™é‡Œä½œè€…é€šè¿‡å®éªŒå‘ç°è®­ç»ƒ $\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t},t)$ æ—¶ç»™æ—¶é—´æˆ³ $t$ èƒ½å–å¾—æ›´å¥½çš„æ•ˆæœï¼ˆå³ $\boldsymbol{\mu}_{\theta}$ æ˜¯ä¸€ä¸ªä¸ä»…å…³äº $\mathbf{x}_{t}$ è¿˜å…³äº $t$ çš„å‡½æ•°ï¼‰ï¼Œè¿™å…¶å®ä¹Ÿç¬¦åˆç›´è§‰ã€‚
> -   æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æ¯ä¸€æ­¥çš„ $\boldsymbol{\mu}_{\theta}$ éƒ½ç†åº”æ˜¯ä¸åŒçš„ï¼Œæ•…å°†æ—¶é—´æˆ³ $t$ ä½œä¸ºæ¨¡å‹è¾“å…¥ä¼ å…¥ï¼Œå¯ä»¥é¿å…è®­ç»ƒ $T$ ä¸ªæ¨¡å‹ï¼Œè€Œç”¨ä¸€ä¸ªæ¨¡å‹è¡¨ç¤ºï¼ˆå…±ç”¨å‚æ•°ï¼‰ã€‚

> [!note]- Proof
>
> å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒ $P = \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_P, \boldsymbol{\Sigma}_P)$ å’Œ $Q = \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_Q, \boldsymbol{\Sigma}_Q)$ï¼Œå®ƒä»¬çš„ KL æ•£åº¦ $D_{KL}(P||Q)$ çš„ä¸€èˆ¬å…¬å¼ä¸ºï¼š
>
> $$
> D_{KL}(P||Q) = \frac{1}{2} \left[ \text{tr}(\boldsymbol{\Sigma}_Q^{-1} \boldsymbol{\Sigma}_P) + (\boldsymbol{\mu}_Q - \boldsymbol{\mu}_P)^T \boldsymbol{\Sigma}_Q^{-1} (\boldsymbol{\mu}_Q - \boldsymbol{\mu}_P) - \log \frac{|\boldsymbol{\Sigma}_P|}{|\boldsymbol{\Sigma}_Q|} - d \right]
> $$
>
> $$
> \begin{aligned}
> \boldsymbol{\Sigma}_Q^{-1} &= (\sigma_t^2 \mathbf{I})^{-1} = \frac{1}{\sigma_t^2} \mathbf{I}\\
> \text{tr}(\boldsymbol{\Sigma}_Q^{-1} \boldsymbol{\Sigma}_P) &= \text{tr}(\frac{1}{\sigma_t^2} \mathbf{I} \cdot \tilde{\beta}_t \mathbf{I}) = \text{tr}(\frac{\tilde{\beta}_t}{\sigma_t^2} \mathbf{I}) = \frac{\tilde{\beta}_t}{\sigma_t^2} \text{tr}(\mathbf{I}) = \frac{\tilde{\beta}_t}{\sigma_t^2} d\\
> (\boldsymbol{\mu}_Q - \boldsymbol{\mu}_P)^T \boldsymbol{\Sigma}_Q^{-1} (\boldsymbol{\mu}_Q - \boldsymbol{\mu}_P) &= (\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) - \tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_t, \mathbf{x}_0))^\top (\frac{1}{\sigma_t^2} \mathbf{I}) (\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) - \tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_t, \mathbf{x}_0)) \\
> &= \frac{1}{\sigma_t^2} ||\tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)||^2\\
> \log \frac{|\boldsymbol{\Sigma}_P|}{|\boldsymbol{\Sigma}_Q|} &= \log \frac{|\tilde{\beta}_t \mathbf{I}|}{|\sigma_t^2 \mathbf{I}|} = \log \frac{(\tilde{\beta}_t)^d}{(\sigma_t^2)^d} = d \log \frac{\tilde{\beta}_t}{\sigma_t^2}\\
> \end{aligned}
> $$
>
> å°†ä»¥ä¸Šè®¡ç®—ç»“æœå¸¦å› KL æ•£åº¦å…¬å¼å¯ä»¥å¾—åˆ°ï¼š
>
> $$
> \begin{aligned}
> L_{t-1}
> =&\frac{1}{2} \left[ \frac{\tilde{\beta}_t}{\sigma_t^2} d + \frac{1}{\sigma_t^2} ||\tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)||^2 - d \log \frac{\tilde{\beta}_t}{\sigma_t^2} - d \right]\\
> =&\mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \frac{1}{2\sigma_t^2} || \tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) ||^2 \right] + \underbrace{\frac{1}{2} \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \frac{\tilde{\beta}_t}{\sigma_t^2} d - d \log \frac{\tilde{\beta}_t}{\sigma_t^2} - d \right]}_{C}\\
> \end{aligned}
> $$
>
> å…¶ä¸­åé¢çš„å¸¸æ•°é¡¹ä¸æ¢¯åº¦æ— å…³ï¼Œæˆ‘ä»¬åœ¨åšæœºå™¨å­¦ä¹ ä»»åŠ¡çš„æ—¶å€™å¯ä»¥ç›´æ¥å¿½ç•¥ã€‚

æ ¹æ®å‰é¢ $q(\mathbf{x}_{t}|\mathbf{x}_{0})$ çš„å¼å­æˆ‘ä»¬æœ‰ï¼š

$$
\mathbf{x}_{t} = \mathbf{x}_{t}(\mathbf{x}_{0}, \boldsymbol{\epsilon}) = \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon},\quad
\text{where}\space \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})
$$

è¿™é‡Œå¼•å…¥äº†å™ªå£°é¡¹ $\boldsymbol{\epsilon}$ã€‚æ³¨æ„ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ$\mathbf{x}_{0}$ æ˜¯å·²çŸ¥çš„ï¼Œåˆ™ $\mathbf{x}_{t}$ æ˜¯å…³äº $\mathbf{x}_{0}$ å’Œ $\boldsymbol{\epsilon}$ çš„å‡½æ•°ï¼ˆæ‰€ä»¥è¡¨ç¤ºä¸º $\mathbf{x}_{t}(\mathbf{x}_{0}, \boldsymbol{\epsilon})$ï¼‰ï¼›è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œ$\mathbf{x}_{t}$ æ˜¯å·²çŸ¥çš„ï¼Œ$\mathbf{x}_{0}$ ç¡®æ˜¯æœªçŸ¥çš„ï¼ˆæ‰€ä»¥è®­ç»ƒé¡¹æ˜¯ $\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t},t)$ï¼Œå…³äº $\mathbf{x}_{t}$ å’Œ $t$ çš„å‡½æ•°ï¼‰ã€‚å› æ­¤ $L_{t-1}$ ä¸­çš„ $\mathbf{x}_{0}$ ä¸€é¡¹éœ€ç”¨ $\mathbf{x}_{0} = \dfrac{1}{\sqrt{\bar{\alpha}_{t}}} \mathbf{x}_{t}(\mathbf{x}_{0},\boldsymbol{\epsilon}) - \dfrac{\sqrt{1-\bar{\alpha}_{t}} }{\sqrt{\bar{\alpha}_{t}} } \boldsymbol{\epsilon}$ æ›¿æ¢ï¼Œå¦‚æ­¤å¯å¾—ï¼š

> æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å…ˆå‰åªæ˜¯å¾—åˆ°äº† $q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})$ï¼Œæ—¢ç„¶å¤šä¸€ä¸ª $\mathbf{x}_{0}$ æ²¡æ³•åšï¼Œæˆ‘ä»¬åœ¨æå‡ºå™ªå£°é¡¹çš„æƒ…å†µä¸‹å°±å¯ä»¥æŠŠ $\mathbf{x}_{0}$ ç”¨å« $\mathbf{x}_{t}$ çš„å¼å­ä»£æ¢ï¼Œä»è€Œå¾—åˆ°äº† $\mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})}\left[ q(\mathbf{x}_{t-1}|\mathbf{x}_t) \right]$ï¼Œè¿™ä¸ªå¼å­å°±å¯ä»¥åº”ç”¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº†ï¼Œæ•…æˆ‘ä»¬æŠŠè¿™ä¸ªå¼å­ä½œä¸ºçœŸæ­£çš„ç›®æ ‡å‡½æ•°ã€‚å½“ç„¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ $\mathbf{x}_{t}$ æœ¬èº«ä¹Ÿæ˜¯ç”± $\mathbf{x}_{0}$ å’Œ $\mathbf{\epsilon}$ æ‰€å¾—åˆ°çš„ï¼Œæ•…è®°ä¸º $\mathbf{x}_{t}(\mathbf{x}_{0},\boldsymbol{\epsilon})$ã€‚

$$
\begin{aligned}
\mathcal{L}_{t - 1} - C &= {\mathbb{E}}_{{\mathbf{x}}_{0},\boldsymbol{\epsilon} }\left\lbrack  {\frac{1}{2{\sigma }_{t}^{2}}{\begin{Vmatrix}{\widetilde{\mathbf{\mu }}}_{t}\left( {\mathbf{x}}_{t}\left( {\mathbf{x}}_{0},\boldsymbol{\epsilon}\right) ,\dfrac{1}{\sqrt{{\bar{\alpha }}_{t}}}\left( {\mathbf{x}}_{t}\left( {\mathbf{x}}_{0},\boldsymbol{\epsilon}\right)  - \sqrt{1 - {\bar{\alpha }}_{t}}\boldsymbol{\epsilon}\right) \right)  - {\mathbf{\mu }}_{\theta }\left( {\mathbf{x}}_{t}\left( {\mathbf{x}}_{0},\boldsymbol{\epsilon}\right) ,t\right) \end{Vmatrix}}^{2}}\right\rbrack\\
 &= {\mathbb{E}}_{{\mathbf{x}}_{0},\boldsymbol{\epsilon}}\left\lbrack  {\frac{1}{2{\sigma }_{t}^{2}}{\begin{Vmatrix}\dfrac{1}{\sqrt{{\alpha }_{t}}}\left( {\mathbf{x}}_{t}\left( {\mathbf{x}}_{0},\boldsymbol{\epsilon}\right)  - \dfrac{{\beta }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}\boldsymbol{\epsilon}\right)  - {\mathbf{\mu }}_{\theta }\left( {\mathbf{x}}_{t}\left( {\mathbf{x}}_{0},\boldsymbol{\epsilon}\right) ,t\right) \end{Vmatrix}}^{2}}\right\rbrack
\end{aligned}
$$

> [!note]- Proof
>
> æˆ‘çœ‹å¾ˆå¤šåœ°æ–¹éƒ½æ²¡æœ‰è¿™ä¸€æ®µçš„æ¨å¯¼ï¼Œå…¶å®å°±æ˜¯æš´åŠ›ä»£è¿›å»ç„¶ååˆ©ç”¨å‰é¢ $\bar{\alpha}_{t}$ çš„å®šä¹‰å¼æ¥æš´è§£ã€‚
>
> $$
> \begin{aligned}
> \tilde{\boldsymbol{\mu}}_{t}&= \dfrac{1}{1-\bar{\alpha}_{t}} \left( \sqrt{\bar{\alpha}_{t-1}} (1-\alpha_{t}) \mathbf{x}_{0} + \sqrt{\alpha_{t}} (1-\bar{\alpha}_{t-1}) \mathbf{x}_{t}   \right) \\
> &= \dfrac{1}{1-\bar{\alpha}_{t}} \left( \sqrt{\bar{\alpha}_{t-1}} (1-\alpha_{t}) \left( \dfrac{1}{\sqrt{\bar{\alpha}_{t}}} \mathbf{x}_{t}(\mathbf{x}_{0},\boldsymbol{\epsilon}) - \dfrac{\sqrt{1-\bar{\alpha}_{t}} }{\sqrt{\bar{\alpha}_{t}} } \boldsymbol{\epsilon}\right) + \sqrt{\alpha_{t}}  (1-\bar{\alpha}_{t-1})\mathbf{x}_{t} \right)
> \end{aligned}
> $$
>
> è¿™é‡Œä»¥æ¨å¯¼ $\mathbf{x}_{t}$ é¡¹çš„ç³»æ•°ä¸ºä¾‹ï¼Œ$\boldsymbol{\epsilon}$ é¡¹çš„æ¨å¯¼çœç•¥ï¼š
>
> $$
> \begin{aligned}
> &\dfrac{1}{1-\bar{\alpha}_{t}} \left( \dfrac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})}{\sqrt{\bar{\alpha}_{t}} } + \sqrt{\alpha_{t}} (1-\bar{\alpha}_{t-1})  \right) \\
> =& \dfrac{1}{1-\bar{\alpha}_{t}} \left( \dfrac{1}{\sqrt{\alpha_{t}}} - \sqrt{\alpha_{t}} + \sqrt{\alpha_{t}} + \sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})   \right) \\
> =& \dfrac{\sqrt{\alpha_{t}}}{1-\bar{\alpha}_{t}} \left( \dfrac{1}{\alpha_{t}}- \bar{\alpha}_{t-1} \right)    \\
> =& \dfrac{\sqrt{\alpha_{t}} }{1-\bar{\alpha}_{t}} \cdot \dfrac{1-\bar{\alpha}_{t}}{\alpha_{t}} \\
> =& \dfrac{1}{\sqrt{\alpha_{t}} }
> \end{aligned}
> $$

è€Œå› ä¸º $\mathbf{x}_{t}$ å…¶å®æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼Œç›´è§‰æ˜¯è®©æˆ‘ä»¬çš„æ¨¡å‹ $\boldsymbol{\mu}_{\theta}$ å»å­¦ä¹ å™ªå£°é¡¹ï¼Œå³ $\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t},t)$ ä¸­ $\mathbf{x}_{t}$ é¡¹çš„ç³»æ•°åº”è®¾å®šä¸º $\dfrac{1}{\sqrt{\alpha_{t}}}$ï¼Œå³å’Œ $\tilde{\boldsymbol{\mu}}_{t}$ é¡¹ä¸­ $\mathbf{x}_{t}$ é¡¹çš„ç³»æ•°ç›¸åŒï¼Œè¯¦è§æŠµæ¶ˆã€‚æ•…å¯ä»¥è¡¨ç¤ºæˆè®­ç»ƒæ¨¡å‹ $\boldsymbol{\epsilon}_{\theta}$ æ¥å­¦ä¹ å™ªå£°é¡¹ $\boldsymbol{\epsilon}$ï¼Œå³è®¾ï¼š

$$
\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t) = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{\mathbf{x}}_{t} - \frac{{\beta }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{\boldsymbol{\epsilon}}_{\theta }\left( {{\mathbf{x}}_{t},t}\right) }\right)
$$

æ¶ˆæ‰ $\mathbf{x}_{t}$ å¹¶æå‡ºå™ªå£°é¡¹çš„ç³»æ•°ï¼Œå¾—åˆ°è¿›ä¸€æ­¥ç®€åŒ–çš„æŸå¤±å‡½æ•°ï¼š

$$
\mathcal{L}_{t-1} - C = {\mathbb{E}}_{{\mathbf{x}}_{0},\boldsymbol{\epsilon}}\left\lbrack  {\frac{{\beta }_{t}^{2}}{2{\sigma }_{t}^{2}{\alpha }_{t}\left( {1 - {\bar{\alpha }}_{t}}\right) }{\begin{Vmatrix}\boldsymbol{\epsilon} - {\boldsymbol{\epsilon}}_{\theta }\left( \sqrt{{\bar{\alpha }}_{t}}{\mathbf{x}}_{0} + \sqrt{1 - {\bar{\alpha }}_{t}}\boldsymbol{\epsilon},t\right) \end{Vmatrix}}^{2}}\right\rbrack
$$

æ­¤å¤–è¿˜æœ‰ $\mathcal{L}_{0} = -\log p_{\theta}(\mathbf{x_{0}|\mathbf{x}_{1}})$ çš„ä¸€é¡¹æ²¡æœ‰å¤„ç†ï¼Œè®ºæ–‡çš„åšæ³•æ˜¯ï¼š

![|695](https://img.memset0.cn/2025/02/16/WyfS9cAL.png)

## 2. Implementation

### 2.1. Training

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¿…æšä¸¾æ¯ä¸ª $t$ã€‚å› ä¸ºä¸Šé¢å·²ç»æ¨å¯¼å‡ºäº†æ¯ä¸€æ—¶åˆ»çš„åŠ å™ªç»“æœçš„å°é—­å½¢å¼ï¼Œè¿™é‡Œå¯ä»¥ç›´æ¥å‡åŒ€éšæœºä¸€ä¸ª $t$ï¼Œå¹¶è®¡ç®—è¿™ä¸€æ—¶åˆ»çš„æŸå¤±ï¼Œè¿™æ ·å°±èƒ½ä¿è¯æ¯ä¸ªæ—¶åˆ» $t$ éƒ½èƒ½è®­ç»ƒåˆ°ä¸€å®šæ ·æœ¬ï¼Œå¹¶é™ä½è®­ç»ƒå¼€é”€ã€‚

![|360](https://img.memset0.cn/2025/02/15/5RN0uwuG.png)

### 2.2. Sampling

è¿™é‡Œæˆ‘ä»¬éœ€è¦æ²¿ç€åå‘é©¬å°”ç§‘å¤«è¿‡ç¨‹æ¨¡æ‹Ÿæ¯ä¸€æ­¥ï¼ŒåŒæ—¶é€æ¸å°†æ¯ä¸€æ­¥çš„å™ªå£°é¡¹å‡å»ï¼Œè¿™ä¸€å™ªå£°é¡¹æ˜¯é€šè¿‡æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹çš„ç»“æœã€‚

![|360](https://img.memset0.cn/2025/02/15/aQ8WBYCD.png)

## 3. References

- åŸå§‹è®ºæ–‡
- [ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¼«è°ˆï¼ˆä¸€ï¼‰ï¼šDDPM = æ‹†æ¥¼ + å»ºæ¥¼ - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://kexue.fm/archives/9119)
- [å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆä¸€ï¼‰ï¼šåŸæ¥æ˜¯è¿™ä¹ˆä¸€å›äº‹ - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/5253)
- [ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¼«è°ˆï¼ˆäºŒï¼‰ï¼šDDPM = è‡ªå›å½’å¼ VAE - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/9152)
- [Diffusion æ‰©æ•£æ¨¡å‹å¤§ç™½è¯è®²è§£ï¼Œçœ‹å®Œè¿˜ä¸æ‡‚ï¼Ÿä¸å¯èƒ½ï¼ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/610012156)
- [lucidrains/denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)
- [æ‰©æ•£æ¨¡å‹ä¹‹ DDPM - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/563661713)
- [What are Diffusion Models? | Lil'Log](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [å°ç™½ä¹Ÿå¯ä»¥æ¸…æ™°ç†è§£ diffusion åŸç†: DDPM - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/693535104)

<!-- begin-private-notes -->

%% end notes %%

## 4. Word Table

| Word | Explain |
| ---: | :------ |

## 5. Annotations

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2&annotation=CAAPGHZT" style="color:inherit!important;text-decoration:none!important"><span style="color:#2ea8e5;background:#2ea8e540;border-radius:2px">CAAPGHZT</span> A diffusion probabilistic model (which we will call a â€œdiffusion modelâ€ for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.</a>

ğŸ”¤ æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆæˆ‘ä»¬ç®€ç§°ä¸ºâ€œæ‰©æ•£æ¨¡å‹â€ï¼‰æ˜¯ä¸€ä¸ªå‚æ•°åŒ–çš„é©¬å°”å¯å¤«é“¾ï¼Œé€šè¿‡å˜åˆ†æ¨æ–­è¿›è¡Œè®­ç»ƒï¼Œä»¥åœ¨æœ‰é™æ—¶é—´å†…ç”Ÿæˆä¸æ•°æ®åŒ¹é…çš„æ ·æœ¬ã€‚è¯¥é“¾çš„è½¬ç§»æ˜¯å­¦ä¹ åˆ°çš„ï¼Œä»¥é€†è½¬æ‰©æ•£è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æ˜¯ä¸€ä¸ªé€æ¸å‘æ•°æ®æ·»åŠ å™ªå£°çš„é©¬å°”å¯å¤«é“¾ï¼Œæ²¿ç€ä¸é‡‡æ ·ç›¸åçš„æ–¹å‘ï¼Œç›´åˆ°ä¿¡å·è¢«ç ´åã€‚å½“æ‰©æ•£ç”±å°é‡çš„é«˜æ–¯å™ªå£°ç»„æˆæ—¶ï¼Œåªéœ€å°†é‡‡æ ·é“¾çš„è½¬ç§»è®¾ç½®ä¸ºæ¡ä»¶é«˜æ–¯ï¼Œè¿™æ ·å¯ä»¥å®ç°ç‰¹åˆ«ç®€å•çš„ç¥ç»ç½‘ç»œå‚æ•°åŒ–ã€‚ğŸ”¤

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2&annotation=5M6N58EL" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">5M6N58EL</span> pÎ¸(x0:T ) is called the reverse process,</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2&annotation=PB8TWSFT" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">PB8TWSFT</span> Markov chain</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2&annotation=XW3KLGL2" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">XW3KLGL2</span> q(xt|xtâˆ’1) := N (xt; âˆš1 âˆ’ Î²txtâˆ’1, Î²tI)</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2&annotation=IQX9TIXY" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">IQX9TIXY</span> Î±t := 1 âˆ’ Î²t and Î± Ì„t := âˆt s=1 Î±s, we have q(xt|x0) = N (xt; âˆšÎ± Ì„tx0, (1 âˆ’ Î± Ì„t)I)</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=3&annotation=KUEE7RM5" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">KUEE7RM5</span> q(xtâˆ’1|xt, x0) = N (xtâˆ’1; ÌƒÎ¼t(xt, x0), Î² ÌƒtI), (6) where ÌƒÎ¼t(xt, x0) := âˆšÎ± Ì„tâˆ’1Î²t 1 âˆ’ Î± Ì„t x0 + âˆš</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=3&annotation=MC7GNQHF" style="color:inherit!important;text-decoration:none!important"><span style="color:#a28ae5;background:#a28ae540;border-radius:2px">MC7GNQHF</span> p</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=3&annotation=4LKZTXAT" style="color:inherit!important;text-decoration:none!important"><span style="color:#2ea8e5;background:#2ea8e540;border-radius:2px">4LKZTXAT</span> We ignore the fact that the forward process variances Î²t are learnable by reparameterization and instead fix them to constants</a>

ğŸ”¤ æˆ‘ä»¬å¿½ç•¥äº†å‰å‘è¿‡ç¨‹æ–¹å·® Î²t å¯ä»¥é€šè¿‡é‡å‚æ•°åŒ–å­¦ä¹ çš„äº‹å®ï¼Œè€Œæ˜¯å°†å®ƒä»¬å›ºå®šä¸ºå¸¸æ•°ã€‚ğŸ”¤

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=3&annotation=PAYBHELR" style="color:inherit!important;text-decoration:none!important"><span style="color:#2ea8e5;background:#2ea8e540;border-radius:2px">PAYBHELR</span> The first choice is optimal for x0 âˆ¼ N (0, I), and the second is optimal for x0 deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance</a>

ğŸ”¤ ç¬¬ä¸€ä¸ªé€‰æ‹©å¯¹äº x0 âˆ¼ N (0, I) æ˜¯æœ€ä¼˜çš„ï¼Œç¬¬äºŒä¸ªé€‰æ‹©å¯¹äº x0 ç¡®å®šæ€§è®¾ç½®ä¸ºä¸€ä¸ªç‚¹æ˜¯æœ€ä¼˜çš„ã€‚è¿™ä¸¤ç§æç«¯é€‰æ‹©åˆ†åˆ«å¯¹åº”äºå…·æœ‰åæ ‡å•ä½æ–¹å·®çš„æ•°æ®çš„é€†è¿‡ç¨‹ç†µçš„ä¸Šç•Œå’Œä¸‹ç•Œã€‚ğŸ”¤

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=4&annotation=T238PN4T" style="color:inherit!important;text-decoration:none!important"><span style="color:#2ea8e5;background:#2ea8e540;border-radius:2px">T238PN4T</span> to</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=4&annotation=PXJUEADS" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">PXJUEADS</span> To summarize, we can train the reverse process mean function approximator Î¼Î¸ to predict ÌƒÎ¼t, or by modifying its parameterization, we can train it to predict . (There is also the possibility of predicting x0, but we found this to lead to worse sample quality early in our experiments.</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=5&annotation=YG5CAYB3" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">YG5CAYB3</span> Unconditional CIFAR10 reverse process parameterization and training objective ablation. Blank entries were unstable to train and generated poor samples with out-ofrange scores.</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=6&annotation=IEMCZQ34" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">IEMCZQ34</span> We find that the baseline option of predicting ÌƒÎ¼ works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14)</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=6&annotation=NPER9S5U" style="color:inherit!important;text-decoration:none!important"><span style="color:#facd5a;background:#facd5a40;border-radius:2px">NPER9S5U</span> but much better when trained with our simplified objective.</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p2x108y315</span> pÎ¸(x0:T ) is called the reverse process,</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p2x353y315</span> ï¿½ âˆ¼ Markov chain w</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p2x295y202</span> q(xt |xtâˆ’1 ) := N (xt ; 1 âˆ’ Î²t xtâˆ’1 , Î²t I)</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=2" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p2x321y73</span> n Î±t := 1 âˆ’ Î²t and á¾±t := s=1 Î±s , x , (1 âˆ’ á¾± )I)</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=3" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p3x154y599</span> q(xtâˆ’1 |xt , x0 ) = N (xtâˆ’1 ; Î¼Ìƒt (xt , x0 ), Î²Ìƒt I), âˆš âˆš</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=4" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p4x107y386</span> To summarize, we can train the reverse process mean function approximator ÂµÎ¸ to predict ËœÂµt, or by modifying its parameterization, we can train it to predict Ïµ. (There is also the possibility of predicting x0, but we found this to lead to worse sample quality early in our experiments.)</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=5" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p5x343y651</span> Unconditional CIFAR10 reverse process parameterization and training objective ablation. Blank entries were unstable to train and generated poor samples with out-ofrange scores.</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=6" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p6x108y352</span> We find that the baseline option of predicting ËœÂµ works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14).</a>

> <a href="zotero://open-pdf/library/items/V37FIZ93?page=6" style="color:inherit!important;text-decoration:none!important"><span style="color:#f9cd59;background:#f9cd5940;border-radius:2px">highlight-p6x107y308</span> but much better when trained with our simplified objective.</a>

## 6. Questions

- P2. [E [âˆ’ log pÎ¸(x0)] â‰¤ Eq [ âˆ’ log pÎ¸(x0:T ) q(x1:T |x0) ]](zotero://open-pdf/library/items/V37FIZ93?page=2&annotation=QVSD4QRV)
- P3. [Efficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L (3) as: Eq [ DKL(q(xT |x0) â€– p(xT )) } {{ } LT + âˆ‘ t>1 DKL(q(xtâˆ’1|xt, x0) â€– pÎ¸(xtâˆ’1|xt)) } {{ } Ltâˆ’1 âˆ’ log pÎ¸(x0|x1) } {{ } L0 ]](zotero://open-pdf/library/items/V37FIZ93?page=3&annotation=WEI7TBZT)
- P3. [stochastic](zotero://open-pdf/library/items/V37FIZ93?page=3&annotation=ELK4K7RG)
- P14. [All models have two convolutional residual blocks per resolution level and self-attention blocks at the 16 Ã— 16 resolution between the convolutional blocks [6]. Diffusion time t is specified by adding the Transformer sinusoidal position embedding [60] into each residual block.](zotero://open-pdf/library/items/V37FIZ93?page=14&annotation=U3YU73JE)
- P14. [Ã— All models have two convolutional residual blocks at the 16 Ã— 16 resolution between the convolutional Ã— Ã— per resolution level and self-attention blocks at the 16 Ã— 16 resolution between the convolutional blocks [6]. Diffusion time t is specified by adding the Transformer sinusoidal position embedding [60] Ã— blocks [6]. Diffusion time t is specified by adding the Transformer sinusoidal position embedding [60] into each residual block.](zotero://open-pdf/library/items/V37FIZ93?page=14)

## 7. Marks

<!-- end-private-notes -->

%% Import Date: 2025-02-23T01:59:00.698+08:00 %%
