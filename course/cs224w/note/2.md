---
title: 经典工作
sync: /course/cs224w/note/2.md
---

## 1. Lecture 2: Traditional Methods for ML on Graphs

### 1.1. Node-Level Prediction

> Goal: characterize the structure and position of a node in the network. `找到能够描述节点在网络中结构与位置的特征。`

#### 1.1.1. Node Centrality

**节点中心性(node centrality)**

- **eigenvector centrality**：认为如果节点邻居重要，那么节点本身也重要。$c_v = \dfrac{1}{\lambda} \sum_{u\in N(v)} c_u \Longrightarrow \lambda \boldsymbol c = \boldsymbol{Ac}$（$\lambda$ 是正常数）
- **betweenness centrality**：认为如果一个节点处在很多节点对的最短路径上，那么这个节点是重要的。$c_v=\displaystyle\sum_{s\neq t\neq v}\dfrac{\#\text{(s 和 t 之间包含 v 的最短路径)}}{\#\text{(s 和 t 之间的最短路径)}}$。
- **closeness centrality**：认为如果一个节点距其他节点的距离最短，那么认为这个节点是重要的。$c_v = \dfrac{1}{\sum_{u\neq v} \#\text{(u 和 v 之间的最短距离)}}$。

#### 1.1.2. Clustering Coefficient

**聚类常数(clustering coefficient)**：衡量节点邻居的连接程度。描述节点的局部结构信息。

$$
e_v = \dfrac{\#\text{(邻居之间的边的条数)}}{\binom{k_v}{2}} \in [0,1]
$$

![wollrRaH.png|324](https://img.memset0.cn/2024/07/30/wollrRaH.png)

- Observation: Clustering Coefficient 相当于数 ego-network 中的**三角形(triangle)** 的**导出子图(induced subgraph)** 的数量。这里的有根三角形就是一种 graphlet（$G_2$）。
- **平均聚类常数(average clustering coefficient)** $e=\dfrac{1}{N} \displaystyle\sum_{i=1}^n e_i$ 使我们能够看到网络的某些部分是否出现更密集的边。在社交网络中，平均聚类常数往往非常高。

#### 1.1.3. Graphlets

**Graphlets**: ==rooted== ==connected== non-isomorphic subgraphs.

![cAbcIJ3I.png|520](https://img.memset0.cn/2024/07/30/cAbcIJ3I.png)

**图元度向量(Graphlet Degree Vector, GDV)**: A count vector of graphlets rooted at a given node. `统计某些图元出现次数的向量。`

### 1.2. Link-Level Prediction

#### 1.2.1. Distance-Based Features

用两点间最短路来描述两个点之间的关系。

![QQ_1722318468269.png|308](https://img.memset0.cn/2024/07/30/q5fSVKJ5.png)

#### 1.2.2. Local Neighborhood Overlap

如果用最短路来描述节点之间的关联，将不能捕捉到**邻域重叠(local neighborhood overlap)** 的程度：$(B,H)$ 的领域交为 $2$，相比于 $(B,E)$ 和 $(A,B)$，他们的联系应该更紧密，但最短路并不能反应这一点。

一些考虑了邻域的系数如下，用 $N(x)$ 表示节点 $x$ 的领域：

- Common neighbor: $|N(v_1)\cap N(v_2)|$。
- Jaccard's coefficient: $\dfrac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}$，归一化后的结果。
- Adamic-Adar index: $\displaystyle\sum_{u\in N(v_1)\cap N(v_2)} \dfrac{1}{\log k_u}$，这里 $k_u$ 是点 $u$ 的度数。在社交网络上表现良好：有一堆度数低的共同好友比有一堆名人共同好友的得分更高。

但如果两个点的距离 $>2$，以上统计邻域的方法就会失效。

#### 1.2.3. Global Neighborhood Overlap

**卡茨指标(Katz index)**: 统计两点直接的路径条数 _count the number of walks of all lengths between a given pair of nodes_

- 可以通过**邻接矩阵(adjacent matrix)** 的幂次计算。

$$
S_{v_1 v_2} = \sum_{l=1}^\infty \beta^l \boldsymbol{A}^l_{v_1 v_2} = (\boldsymbol I-\beta \boldsymbol A)^{-1} - \boldsymbol I
$$

- discount factor: $\beta \in (0,1)$。距离更长的路径对关联性的影响应当越小。
- 可以通过**封闭形式(closed-form)** 计算 Katz index matrix。

### 1.3. Graph-Level Prediction

> Goal: We want features that characterize the structure of an entire graph.

#### 1.3.1. Kernal Methods

**核方法(kernal methods)** 在传统的 ML for graph-level predicton 中广泛应用。

![gfY7uGN0.png|452](https://img.memset0.cn/2024/07/31/gfY7uGN0.png)

- Key Idea: **Bag-of-Words (BoW)** for a graph

以 Bag of node degrees 为例：

![5Z3oupeV.png|458](https://img.memset0.cn/2024/07/31/5Z3oupeV.png)

#### 1.3.2. Graphlets Kernal

如果考虑 Bag of graphlets？这里的 graphlets 和 node-level prediction 部分中的不一样，区别在于：

| Graphlets (Node-level Prediction) | Graphlets (Kernal Methods) |
| --------------------------------- | -------------------------- |
| 有根的 _rooted_                   | 无根的 _unrooted_          |
| 连通的 _connected_                | 可以是不连通的             |

![yraM8XcA.png|420](https://img.memset0.cn/2024/07/31/yraM8XcA.png)

- 需要对向量做**归一化(normalize)** 处理。

- Limitation: 计算代价昂贵，最坏情况是不可避免的，因为 **subgraph isomorphism test**（判断一个图是否是另一个图的子图）是 NP-Hard 的。设最大子图的大小为 $k$，则复杂度为 $O(n \cdot deg^{k-1})$。

#### 1.3.3. Weisfeiler-Lehman Kernal

> Goal: Design an ==efficient== graph feature descriptor $\Phi(G)$.
>
> Idea: Use neighborhood structure to iteratively enrich node vocabulary.

每次针对邻域结构做一个哈希+离散化，这里截图一下最后一轮。

![ompVO1r2.png|454](https://img.memset0.cn/2024/07/31/ompVO1r2.png)

- 时间复杂度是关于边数**线性(linear)** 的。

## 2. Lecture 3: Node Embeddings

> Goal: Efficient task-independent feature learning for machine learning with graphs! `用一个向量来抓住用于机器学习的、与下游任务无关的特征。`

![NSNoczsQ.png|534](https://img.memset0.cn/2024/07/31/NSNoczsQ.png)

### 2.1. Embedding Framework

> Goal: 在嵌入空间中的相似性（如，使用**点积(dot product)**）近似于原图中的相似性。

#### 2.1.1. Encoder

![l9BIvqR4.png|450](https://img.memset0.cn/2024/07/31/l9BIvqR4.png)

- Simplest encoding approach: Encoder is just an embedding-loopup: $\operatorname{ENC}(v) = \boldsymbol z_v = \boldsymbol Z \cdot v\ (\boldsymbol Z\in \mathbb R^{d\times |V|}, \ v \in \mathbb I ^{|V|})$。

#### 2.1.2. Decoder

- Based on node similarity
- maximize $\boldsymbol z_v^\text T \boldsymbol z_u$ for node pairs $(u,v)$ that are similar.

#### 2.1.3. Framework Summary

![6wmdhMdY.png|460](https://img.memset0.cn/2024/07/31/6wmdhMdY.png)

### 2.2. Random-Walk Embeddings Approach

#### 2.2.1. Notation

- $P(v\mid \boldsymbol z_u)$ 表示从 $u$ 开始**随机游走(random walk)** 到 $v$ 的概率。

![BtMqmCZB.png|570](https://img.memset0.cn/2024/07/31/BtMqmCZB.png)

#### 2.2.2. Why Random Walks?

表达更灵活，更高效。

- **表达性(Expressivity)**: Flexible stochastic definition of node similarity that incorporates both local and higher-order neighborhood information.

  Idea: if random walk starting from node $u$ visits $v$ with high probability, $u$ and $v$ are similar (high-order multi-hop information)

- **Efficiency**: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks

#### 2.2.3. Unsupervised Feature Learning & Optimization

![lQu401ij.png|439](https://img.memset0.cn/2024/07/31/lQu401ij.png)

==TODO==

### 2.3. Node2Vec

> Goal: Embed nodes with similar network neighborhoods close in the feature space.
>
> Key observation: Flexible notion of network neighborhood $N_R(u)$ of node $u$ leads to rich node embeddings Embed nodes with similar network neighborhoods close in the feature space.

### 2.4. Graph-Level Embedding

#### 2.4.1. Some Easy Approachs

- Approach 1: Run a standard graph embedding technique on the (sub)graph $G$ (node-level), then just sum up (or average). $\boldsymbol z_G= \displaystyle\sum_{v\in G} \boldsymbol{z}_v$. (Simple, but effective.)
- Approach 2: Introduce a “virtual node” to represent the (sub)graph and run a standard graph embedding technique.

  ![QQ_1722516318074.png|353](https://img.memset0.cn/2024/08/01/4jpbilV2.png)

#### 2.4.2. Annoymous Walks Embeddings

![nCMyYkvr.png|365](https://img.memset0.cn/2024/08/01/nCMyYkvr.png)

- 可能的路径条数关于路径长度是指数级增长的，考虑对其进行 sampling。

#### 2.4.3. Learn Walk Embeddings

==TODO==

#### 2.4.4. Hierarchical Embeddings

## 3. Lecture 4: Link Analysis: PageRank

### The "Flow" Model

#### Introduction

![ga22nziq.png|412](https://img.memset0.cn/2024/08/01/ga22nziq.png)

- Define "rank" for node $j$ as $r_j = \displaystyle r_j = \sum_{i\rightarrow j} \dfrac{r_i}{d_i}$.
- The flow equations can be written in matrix formulation as $\boldsymbol{r} = \boldsymbol{M} \cdot \boldsymbol{r}$.

#### Connection to Random Walk

![QQ_1722527726116.png|444](https://img.memset0.cn/2024/08/01/YEmJE7Gw.png)
