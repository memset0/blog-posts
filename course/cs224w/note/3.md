---
title: "III. Introduction to GNN"
sync: /course/cs224w/note/3.md
---

| Lecutre                                  | Content                                                                    | Useful Links                                                                                                                          |
| ---------------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| 6. Graph Neural Networks 1: GNN Model    | <ul><li>Basics of Deep Learning</li><li>Deep Learning for Graphs</li></ul> | https://blog.csdn.net/PolarisRisingWar/article/details/117626483<br><br>https://blog.csdn.net/SongGu1996/article/details/99056721<br> |
| 7. Graph Neural Networks 2: Design Space |                                                                            |                                                                                                                                       |
| 8. Applications of Graph Neural Networks |                                                                            |                                                                                                                                       |
| 9. Theory of Graph Neural Networks       |                                                                            |                                                                                                                                       |

## 1. Basics of Deep Learning

### 1.1. Machine Learning as Optimization

机器学习：可以看成是一个优化问题：$\displaystyle{\min_{\Theta} \mathcal L(\boldsymbol y  ,f(\boldsymbol x))}$。

- 卷积神经网络：做矩阵/栅格（网格图/图片）上的机器学习，主要应用在 CV 中。
- 循环神经网络：做序列（语言）的机器学习，主要应用在 NLP 中。

- 为什么深度学习（用深度神经网络进行机器学习和数据挖掘）？在大数据的表现下更好：

    ![S7kNpG2X.png|347](https://static.memset0.cn/img/v6/2024/08/12/S7kNpG2X.png)

- 为什么图深度学习困难？
    - 图是复杂的，可以有任意规模的节点，往往是极大的。
    - 节点之间没有固定顺序或参考锚点。
    - 网络经常是动态变化的，往往具有**多模态(multimodel)** 特征。

### 1.2. Objective Function

**目标函数(objective function)**：我们训练过程的目标是最小化目标函数。也可以被叫做**损失函数(loss function)**。一些常见的损失函数如下：

- L2 loss：$\mathcal L =\sum_{(\boldsymbol{x},\boldsymbol{y})\in \boldsymbol{\mathcal B}} ||\boldsymbol y-f(\boldsymbol x)||_2$：方便求导，可微分，常用于回归问题。
- **交叉熵(cross entropy)** loss：$\mathcal L = -\sum_i y_i\log P_i$：常用于分类问题。

    ![4547fe3108f6998b2022a4f684da596.png|446](https://static.memset0.cn/img/v6/2024/08/12/YyyL3rnv.png)

### 1.3. Gradient Descent

**梯度下降(gradient descent)**：如果把损失函数看作是一座山脉，如果你要下山，可以每次都沿着高度降低的方向走。

- 需要设置**学习率(learning rate = LR)** $\eta$：
    - 是一个控制每一步学习比例的超参数，可以看做控制每一步下降的步长。
    - **LR scheduling**：控制学习率在训练过程中动态调整（如先快后慢）。
- 在计算梯度时，我们应该采用**分析方法(analytic methods)** 而不是**数值方法(numerical methods)**。不过数值方法可以辅助我们进行验证（`torch.autograd.gradcheck` 就实现了这个功能。）
- 理想状态是到达 $\boldsymbol 0$ 梯度的点，有可能落在鞍点上等问题。

**随机梯度下降(stochastic gradient descent = SGD)**：每次喂一小批数据（称作 mini-batch）$\mathcal{\boldsymbol B}$ 进去计算损失函数和梯度，从而减少计算成本。

- **batch size**：每次 mini-batch 的数据量
- **iteration** / **step**：指 mini-batch 中的一次训练
- **epoch**：指 mini-batch 的一轮训练，即对整个数据集做一次训练，其中的 iteration 数量即 $\dfrac{\text{\#size(dataset)}}{\text{\#size(batch)}}$。

![x0LrARH0.png|344](https://static.memset0.cn/img/v6/2024/08/12/x0LrARH0.png)

### 1.4. Back-propagation

前文我们提到了目标函数 $\mathcal L(\boldsymbol{y}  ,f(\boldsymbol{x}))$，这里的 $f(\boldsymbol{x})$ 可能非常复杂。我们以 $f(\boldsymbol{x}) = \boldsymbol{W}_2(\boldsymbol{W}_1 \boldsymbol{x})$ 为例，应用链式法则计算梯度则有：$\displaystyle{\nabla_{\boldsymbol{x}} f = \frac{\partial f}{\partial (\boldsymbol W_1 \boldsymbol x)} \cdot \frac{\partial (\boldsymbol W_1 \boldsymbol x)}{\partial \boldsymbol x}}$。所以：

$$
\begin{aligned}
\frac{\partial \mathcal L}{\partial \boldsymbol{W}{_2}} &= \frac{\partial \mathcal L}{\partial f} \cdot \frac{\partial  f}{\partial  \boldsymbol{W}_2}\\
\frac{\partial \mathcal L}{\partial \boldsymbol{W}_1} &= \frac{\partial  \mathcal{ L}}{\partial f} \cdot \frac{\partial f}{\partial \boldsymbol{W}_2}\cdot \frac{\partial \boldsymbol{W}_2}{\partial \boldsymbol{W}_1}
\end{aligned}
$$

**反向传播(back-propagation)** 的构成即通过应用**链式法则(chain rule)** 每次右乘一个转移从而得到原函数的梯度。（TBD：这一部分是从课件里超过来的，感觉我理解的不是很严谨啊？）

**隐藏层(hidden layer)**：输入 $\boldsymbol{x}$ 的**中间表示(intermediate representation)**。

### 1.6. Multi-layer Perception

**多层感知器(multi-layer perception = MLP)** 是一种**全连接(fully-connected)**（每一层的所有神经元都与上一层的所有输出和下一层的所有输入相接）的多层神经网络。由于我们选用了非线性的激活函数，我们的神经网络就具备了非线性的表达能力。

在实际训练中有一些针对 GNN Layer 的优化技巧：

- **batch normalization**：在初始化时使用更大的学习率，可以稳定神经网络的训练。
- **dropout**：在训练过程中以一定的概率丢掉一部分随机元，可以防止过拟合。（TBD：为什么 dropout 可以防止过拟合？）
- **attenation** / **gating** / and more ...

## 2. Graph Convolutional Networks

> [!summary] Notations
>
> -   $G$ 表示图；$V$ 表示点集。
> -   $\boldsymbol{A}$ 表示邻接矩阵。
> -   $\boldsymbol{X} \in \mathbb R^{m\times |V|}$：包含所有节点特征的矩阵，每个节点的特征是一个 $m$ 维的向量。
> -   $v$：表示一个点。$N(v)$ 表示节点 $v$ 的所有邻居。

### 2.1. ReCap: Node Embeddings

图神经网络的目标是得到节点的**特征表示(feature representation)** / 节点**嵌入(embedding)**，这是一个表示学习问题：从数据中提取最少的必要信息。

- 低维：向量维度远小于节点数。
- 连续：每个元素都是实数。
- 稠密：每个元素都不为 0。

### 2.2. A Naive Approach

将邻接矩阵和特征合并在一起应用在神经网络上？

- 需要 $O(|V|)$ 的参数，一方面过多，另一方面会**过拟合(over-fitting)**。
- 训练出来的神经网络不适用于不同大小的图，没有泛化能力。
- 对节点顺序敏感——我们需要一个即使改变节点顺序，结果也不会变的模型。

![lkeNqXFY.png|739](https://static.memset0.cn/img/v6/2024/08/13/lkeNqXFY.png)

这就是我们即将介绍的 GCN 和 GraphSAGE。

### 2.3. ?

**图卷积网络(graph convolutional network = GCN)**：将作用在图片/网格上的**卷积神经网络(convolutional neural network)** 泛化到图上。

![7HFMwNoZ.png|624](https://static.memset0.cn/img/v6/2024/08/13/7HFMwNoZ.png)

![9xBO2TJ0.png|318](https://static.memset0.cn/img/v6/2024/08/13/9xBO2TJ0.png)

我们通过邻居**聚合(aggregation)** 的方法取代 CNN 中的 $3\times 3$ filter 来得到节点的嵌入表示。常见的聚合方式有 sum/average/min 等。

![XjpnMXVk.png|357](https://static.memset0.cn/img/v6/2024/08/13/XjpnMXVk.png)

相比于 CNN，我们 GCN 的聚合方法需要满足**置换不变性(permutation invariant)**，我们称我们需要学习的函数 $f$ 是**置换不变函数(permutation invariant function)**。

在每一层的迭代中，所有节点共用的是同一个神经网络（共用同一组参数）：

![Y6PGpKKJ.png|509](https://static.memset0.cn/img/v6/2024/08/13/Y6PGpKKJ.png)

这里我们的**计算图(compute graph)** 可以是任意层数（不是说多层感知器的层数）的，我们一般选择一个有限的常数 $k$，只跑 $k$ 层（即只考虑了 $k$-邻域）。

- 在实际的网络中，取一个 $k$ 较小的值时已经得到了足够多的信息，这就是著名的六度空间理论。
- 如果选择的 $k$ 过大，还可能出现**过平滑(over-smoothing)** 的问题，即所有的节点都输出了同样的结果。

![WFK5H1ZS.png|332](https://static.memset0.cn/img/v6/2024/08/13/WFK5H1ZS.png)

$h_v^{(l)}$ 是第 $l$ 层节点 $v$ 的隐藏表示向量。在第 $l$ 层我们需要训练学习得到的权重参数为 $W_l$（邻域节点聚合的权重）和 $B_l$（节点自身的权重）

![SsEf89AM.png|498](https://static.memset0.cn/img/v6/2024/08/13/SsEf89AM.png)

### 2.4. Matrix Formulation

许多聚合操作可以被表示成稀疏矩阵运算的形式。如：$\displaystyle{\sum_{u\in N(v)} \frac{h_{u}^{(k-1)}}{|N(v)|} \Longrightarrow D^{-1} A H^{(k)}}$。这里的 $D^{-1}A$ 是一个 row normalized matrix。但这种矩阵只考虑了 $v$ 的度数信息，没有考虑 $u$（邻域节点）的度数信息。

为了解决这一问题，可以使用 $\tilde{A} = D^{-\frac 12} A D^{-\frac 12}$，这是一个 symmetric normalized matrix。$\tilde{A}_{ij} = \dfrac{1}{\sqrt{d_i} \cdot \sqrt{d_j}}$，他也被叫做 normalized diffusion matrix。

- 如果相邻两个节点 $(i,j)$ 的度数都很大，则 $\tilde{A}$ 会变得很稠密。
- 对于 $k\in\mathbb Z_+$，都有 $\tilde{A}^k$ 的所有**特征值(eigenvalue)** $\lambda \in [-1,1]$，且最大特征值为 $\lambda=1$。

![jqT56oRX.png|475](https://static.memset0.cn/img/v6/2024/08/13/jqT56oRX.png)

### 2.5. Supervised Training: Classification

> Task: 每个节点是一个药物，判断每个节点是不是有毒。

使用交叉熵损失函数。

![3OulkRny.png|548](https://static.memset0.cn/img/v6/2024/08/14/3OulkRny.png)

节点嵌入向量在向量空间的相似度就反应了节点的相似度：

![KzneuomS.png|456](https://static.memset0.cn/img/v6/2024/08/14/KzneuomS.png)

### 2.6. Unsupervised Training

用节点自身的信息作为标注，在原图中相似的节点应该有相似的节点嵌入。

$$
\mathcal L= \sum_{z_u,z_v} \operatorname{CE}(y_{u,v}, \operatorname{DEC}(z_u,z_v))
$$

- $y_{u,v}$ 为 $1$ 表示 $u$ 和 $v$ 是相似的，否则为 $0$。
- $\operatorname{CE}$ 表示交叉熵损失函数。
- 节点相似度可以是各种方法得到的：random walks / matrix factorization / node proximity in the graph。

### 2.7. GNN vs. "Shallow" Encoding

> [!note] 直推式学习 vs. 归纳式学习
>
> **直推式学习(transductive learning)**：用于预测的节点在训练时就见过。随机游走方法：DeepWalk、Node2Vec。
>
> **归纳式学习(inductive learning)**：用于预测的节点在训练时没见过（需要泛化到新节点）。图神经网络方法：GCN、GraphSAGE、GAT、GIN。

最简单的 encoding 的实现就是进行一个查表，这具有以下问题：

- 需要 $O(|V|)$ 个参数，不方便进行泛化。
- 不能捕获节点的结构信息和**结构相似度(structural similarity)**：结构上相似，位置上远离（直接进行随机游走有长度限制）。
- 只能利用图的结构信息，而不能利用节点和边的属性信息。

![P2LwZF2Y.png|461](https://static.memset0.cn/img/v6/2024/08/14/P2LwZF2Y.png)

### 2.8. GNN vs. CNN

![|755](https://static.memset0.cn/img/v6/2024/08/14/a4PgIcYr.png)

- CNN 的卷积核权重需学习得到；GCN 的卷积核权重由 $\tilde{A}$ 定义。
- CNN 不具有置换不变性；GCN 具有置换不变性——主要是通过聚合方法的选择得到的。

### 2.9. GNN vs. Transformer

Transformer 的核心功能就是引入了**自注意力(self-attention)** 机制：每两个 token / word 之间都会相互影响。这类似于我们 GNN 中每两个点都会相互影响。

## 3. GraphSAGE

![WNFOo364.png|682](https://static.memset0.cn/img/v6/2024/08/14/WNFOo364.png)

---

把 $d$ 维向量投影到二维平面的效果：

![BtxmonWD.png|614](https://static.memset0.cn/img/v6/2024/08/14/BtxmonWD.png)
